<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ben Brostoff</title>
    <description></description>
    <link>http://benbrostoff.github.io/</link>
    <atom:link href="http://benbrostoff.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Decision Trees Introduction (ML Series, Part 4)</title>
        <description>&lt;p&gt;In my continuing attempt to automate as much as of my Daily Fantasy Sports lineup creation as possible, I’ve been exploring decision trees. I realized this technique might be valuable after listening to my thought process as it relates to fantasy sports. Generally, I’ll ask myself questions like “Has this player broken 10 rebounds the last 2 games?” or “Is this player consistently getting over 25 minutes per game?”. In addition to removing bias, a decision tree should ask better questions, improve given more data and generate insights over key features in data.&lt;/p&gt;

&lt;p&gt;I want to start off with the simplest possible example I can think of for a decision tree. I’ll use this example as an opportunity to explore the &lt;code&gt;sklearn.tree&lt;/code&gt; module. In a future post, I’ll review in depth how I constructed my DFS basketball decision tree, but you can &lt;a href=&quot;https://github.com/BenBrostoff/draft-kings-learn/blob/master/recipes/classifier.py&quot;&gt;check it out here if you’re interested&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;&quot;&gt;Part III&lt;/a&gt; of my ML blog series, I reviewed how a neural network could be used to classify some data where only one feature mattered. A similar example here I think will be illustrative. I often find it easier to make the example as close to real as possible, so let’s assume we have data on a group of basketball players, and we want to label them &lt;code&gt;0&lt;/code&gt; if they scored under 20 points and &lt;code&gt;1&lt;/code&gt; if they scored over 20 in a game. The three features in the data set are &lt;code&gt;minutes&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt;; let’s assume only &lt;code&gt;minutes&lt;/code&gt; matters. Below is a simple decision tree with a fake dataset:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/e0269da7acd2d3c98859fe9a98e3ed96.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;You can see in the code snippet above that &lt;code&gt;tree.DecisionTreeClassifier&lt;/code&gt; ships with a &lt;code&gt;feature_importances_&lt;/code&gt; that lists the weight of each feature. Because &lt;code&gt;minutes&lt;/code&gt; is the only feature that matters, it’s assigned a weight of &lt;code&gt;1.0&lt;/code&gt;, while the meaningless &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; features have a weight of &lt;code&gt;0.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/decision_tree_basic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The decision tree correctly identifies that if the player players over 30 minutes a game, then they should score over 20 points (disclaimer: this is an unrealistic and oversimplified example). The &lt;code&gt;graphviz&lt;/code&gt; package allows you to visualize this data:&lt;/p&gt;

&lt;p&gt;The visualization confirms what we know to be true; if a player plays over 30 minutes, they scored over 20 points.&lt;/p&gt;

&lt;p&gt;How does &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; actually work under the hood? The source here is &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py#L75&quot;&gt;pretty difficult at least for me to parse&lt;/a&gt;, so I turned to the blogosphere for answers.&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2018/01/27/decision-trees-intro.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2018/01/27/decision-trees-intro.html</guid>
      </item>
    
      <item>
        <title>Gradient Descent in Simple NN (ML Series, Part 3)</title>
        <description>&lt;p&gt;&lt;em&gt;Note: This post is a WIP. I am leaving it up in its current form for feedback, and will continue to update it, hopefully removing this disclaimer within the week (12/6).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This post builds on &lt;a href=&quot;https://benbrostoff.github.io/2017/10/09/gradient-descent-intuition/&quot;&gt;Part II of my ML series&lt;/a&gt; , and explores what role gradient descent plays in a simple neural network (NN).&lt;/p&gt;

&lt;p&gt;I will be working off the simple one layer NN in Andrew Trask’s blog post &lt;a href=&quot;http://iamtrask.github.io/2015/07/12/basic-python-network/&quot;&gt;A Neural Network in 11 Lines of Python (Part I)&lt;/a&gt;. First, I’m going to review part of Andrew’s posts and explain why this network makes a correct prediction give a certain simple pattern. Next, I’ll create a pattern where the network cannot predict the right answer, and attempt to explain why. In both cases, I will refer back to Part II and build on the purpose of gradient descent. I will be working with &lt;a href=&quot;https://github.com/BenBrostoff/ml-series-source/blob/master/src/3_gradient_descent_in_practice.ipynb&quot;&gt;this IPython notebook&lt;/a&gt;, and it may help to have it open in a separate window.&lt;/p&gt;

&lt;p&gt;For those unfamiliar with Andrew’s blog post, here’s a quick overview. Andrew creates a fake dataset where each data point is a Python list of three elements. If the first element is &lt;code&gt;1&lt;/code&gt;, then the data point should be labeled &lt;code&gt;1&lt;/code&gt;. If not, it’s labeled &lt;code&gt;0&lt;/code&gt;. A quick visual overview of four example inputs and outputs, which will serve as our training set. Later, we can test the model on the remaining 4 of the eight possible examples of this pattern (2 possibles in each slot -&amp;gt; &lt;code&gt;2**3&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/inputs_and_outputs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Simple and Working NN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Andrew implements a simple neural network in Python using only &lt;code&gt;numpy&lt;/code&gt; for some basic utility functions (e.g. taking the dot product of two arrays via &lt;code&gt;np.dot&lt;/code&gt;). I want to be clear on what the terminology “implements a simple neural network” means here. The only goal of this exercise is to get three weights - one for each slot - that can be used to make a prediction. Neural network in this case just refers to the methodology for getting these weights. Because the technique to obtain these weights uses parts that are named neurons and activations and layers and is loosely modeled to look like groups of neurons firing, the technique as a whole is a neural network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 1: Randomize Weights&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The weights could simply be guessed at. Random guessing does play some role in neural networks (importantly, the guessing is not exactly random and is far beyond the scope of this post), and the initial guess at the final weights leverages &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.randn.html&quot;&gt;&lt;code&gt;np.random.randn&lt;/code&gt;&lt;/a&gt;. Note that this network uses &lt;code&gt;np.random.seed(1)&lt;/code&gt; so outputs are consistent. The first guess at the weights computes &lt;code&gt;2 * np.random.random((3, 1)) - 1&lt;/code&gt; - the 2 is simply to scale the weights by 2, which are all between 0 and 1. The &lt;code&gt;-1&lt;/code&gt; then forces any weights that were initialized as less than &lt;code&gt;0.5&lt;/code&gt; to be negative (e.g. &lt;code&gt;2 * 0.4 - 1 == -0.2&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The first guess at the weights suggests they’re &lt;code&gt;-0.17&lt;/code&gt; (too small), &lt;code&gt;0.44&lt;/code&gt; (too big) and &lt;code&gt;-0.99&lt;/code&gt; (too small). Because we know that the first slot is the only one that matters, the correct weights are &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt;, or some large number and two small numbers. A neural network &lt;em&gt;should&lt;/em&gt; move the first weight upward, the second weight downward and the third weight upward.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 2: Guess&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;What is the output when we guess with the weights? The answer to this question is computed via:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;predictions = np.dot(training_set_inputs, weights)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The output here is a &lt;code&gt;numpy&lt;/code&gt; array of shape &lt;code&gt;(4, 1)&lt;/code&gt; (&lt;code&gt;training_set_inputs&lt;/code&gt; - four examples of three elements - is &lt;code&gt;(4, 3)&lt;/code&gt; and the &lt;code&gt;weights&lt;/code&gt; - three “predictors” - are &lt;code&gt;(3, 1)&lt;/code&gt;). It represents guesses at the four labels given the four examples in the training set. Of course, these guesses - &lt;code&gt;-2.5&lt;/code&gt;, &lt;code&gt;2.74&lt;/code&gt;, &lt;code&gt;2.95&lt;/code&gt;, and &lt;code&gt;-2.73&lt;/code&gt; - are quite far away from the actual labels of &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I want to take a quick second to do some discussion of &lt;code&gt;np.dot&lt;/code&gt;. For those with a linear algebra background, feel free to skip this section; I’m an Economics major so &lt;code&gt;np.dot&lt;/code&gt; has taken me some time to wrap my head around. The &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html&quot;&gt;documentation&lt;/a&gt; around &lt;code&gt;np.dot&lt;/code&gt; makes it sound deceivingly simply:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dot product of two arrays. For 2-D arrays it is equivalent to matrix multiplication, and for 1-D arrays to inner product of vectors (without complex conjugation). For N dimensions it is a sum product over the last axis of a and the second-to-last of b:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The matrix math explanation makes the most sense here, and the formula you would apply for matrix math does indeed work:&lt;/p&gt;

&lt;p&gt;That said, I think the intuition here is important - why are we using &lt;code&gt;np.dot&lt;/code&gt;? We know &lt;em&gt;where we want to end up&lt;/em&gt; is four predictions, or a &lt;code&gt;numpy&lt;/code&gt; array of shape &lt;code&gt;(4, 1)&lt;/code&gt;. To get each prediction, we need to apply the weights. Getting the sum weighted prediction would look like the below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First prediction: &lt;code&gt;0 * -0.17 + 0 * 0.44 + 1 * -0.99 == -0.99&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Second prediction: &lt;code&gt;1  * -0.17 + 1 * 0.44 + 1 * -0.99 == -0.73&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Third prediction: &lt;code&gt;1 * -0.17 + 0 * 0.44 + 1 * -0.99 == -1.17&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Fourth prediction:  &lt;code&gt;0 * -0.17 + 1 * 0.44 + 1 * -0.99 == -0.56&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is all &lt;code&gt;np.dot&lt;/code&gt; is doing here. &lt;code&gt;predictions&lt;/code&gt; is &lt;code&gt;[[[-0.99977125], [-0.72507825], [-1.16572724], [-0.55912226]]]&lt;/code&gt;. The linear algebra intuition for &lt;code&gt;np.dot&lt;/code&gt; is a little different, and I have found &lt;a href=&quot;https://www.youtube.com/watch?v=kjBOesZCoqc&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;3Blue1Brown’s linear algebra series&lt;/a&gt; worth watching here (disclaimer: I’m only four videos through). But at least in my minimal machine learning experience, I have found it more useful to stop thinking about rote learning matrix math formulas and referring back to the predictions intuition.&lt;/p&gt;

&lt;p&gt;A little terminology here - &lt;code&gt;predictions&lt;/code&gt; is really the first hidden layer of the neural network, and also the output layer here as the whole network is only a single input and out layer. In &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Andrew Ng’s deeplearning course&lt;/a&gt;, the step is represented as:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Z1 = W1 * X&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where Z1 is the output layer, W1 are weights and X is the training inputs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 3: Apply a Non Linearity and Find Error&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These four numbers from &lt;code&gt;predictions&lt;/code&gt; can now be passed through a non-linearity, or function that does not have the same effect on the output as the input is incremented or decremented. &lt;em&gt;Why&lt;/em&gt; passing &lt;code&gt;predictions&lt;/code&gt; through a non-linearity is important will likely be the subject of another post. For now, I think it’s sufficient to just say that we will pass the four predictions through a function called a sigmoid function that scales them from 0 to 1 and &lt;a href=&quot;http://www.wolframalpha.com/input/?i=sigmoid+function&quot;&gt;looks like the below&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/sigmoid.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, in Andrew Ng’s deeplearning course, this looks like:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;A1 = G(Z1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where G is the non-linearity (sigmoid function in this example). &lt;code&gt;A1&lt;/code&gt; is called a layer of activations, because the non-linearity is thought to “activate neurons” in the loose brain metaphor.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 4: Apply Gradient Descent&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 5: Update the Weights&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 6: Repeat&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Simple and Broken NN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This network breaks when changing the rule for inputs and outputs.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/12/06/gradient-descent-in-simple-nn-draft.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/12/06/gradient-descent-in-simple-nn-draft.html</guid>
      </item>
    
      <item>
        <title>Commits - Not a Proxy for Progress</title>
        <description>&lt;p&gt;I’m resolving to stop using commits as a measure of progress.&lt;/p&gt;

&lt;p&gt;Generating code is easy. Generating code while understanding every line is hard. This may on the surface seem like an absurd statement, especially if new code has no library dependencies. But native code for a specific language has its own complexities. A recent example I ran into - &lt;a href=&quot;https://stackoverflow.com/questions/21771220/error-handling-with-node-js-streams&quot;&gt;error handling with Node &lt;code&gt;stream&lt;/code&gt;s&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Good writers talk all the time about all the work that comes before and after writing. &lt;a href=&quot;https://tim.blog/2016/05/22/sebastian-junger/&quot;&gt;Sebastian Junger&lt;/a&gt; is insistent on understanding his subject matter as much as possible before putting pen to paper. &lt;a href=&quot;https://www.amazon.com/Writing-10th-Anniversary-Memoir-Craft/dp/1439156816&quot;&gt;Stephen King&lt;/a&gt; urges writers to tear apart their initial and even later drafts as the editing process creates insight. Coding should be no different.&lt;/p&gt;

&lt;p&gt;The before and after work for coding should aim to reduce code generated to as small a volume as possible. This process takes time - time to consider different architectures, &lt;a href=&quot;http://benbrostoff.github.io/2017/08/05/rtd/&quot;&gt;time to read documentation&lt;/a&gt; and time to receive and address feedback. In no way does this time correlate with commits, nor would correlation be desirable. Version control should tell the story of what changed in the code, not the story of someone’s coding process. This is the difference between &lt;code&gt;Adds error handling to batch script&lt;/code&gt; and three commits that are different iterations of error handling.&lt;/p&gt;

&lt;p&gt;What can substitute for commits then as a measure of progress on a project? I see value in tools that track the before and after process, in addition to the actual writing of code. I have found &lt;a href=&quot;https://qotoqot.com/qbserve/&quot;&gt;&lt;code&gt;Qbserve&lt;/code&gt; useful here&lt;/a&gt; - it’s a tool that shows time spent in different applications. Good old-fashioned “software journaling” is another technique I love - I especially enjoyed this &lt;a href=&quot;http://winterflower.github.io/2017/08/17/software-engineering-notebook/&quot;&gt;blog post on it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What gets measured gets managed. Let’s make sure we’re measuring the right things.&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/11/09/commits-are-not-a-proxy-for-progress.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/11/09/commits-are-not-a-proxy-for-progress.html</guid>
      </item>
    
      <item>
        <title>Gradient Descent Intuition (ML Series, Part 2)</title>
        <description>&lt;p&gt;How do machines learn?&lt;/p&gt;

&lt;p&gt;One answer is by getting fewer answers wrong over time. One technique for decreasing wrongness is &lt;em&gt;gradient descent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In this post, I want to explore gradient descent as a standalone topic. In the next post in this series, I’ll discuss its relationship with machine learning, and run through a basic example of why gradient descent is useful in helping machines get fewer answers wrong.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gradient Descent - Find The Minimum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matt Nedrich over at Atomic Object has &lt;a href=&quot;https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/&quot;&gt;an excellent definition of gradient descent&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;At a theoretical level, gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved using calculus, taking steps in the negative direction of the function gradient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gradient descent can be thought of as &lt;a href=&quot;http://mathworld.wolfram.com/Gradient.html&quot;&gt;a synonym for slope&lt;/a&gt;. Since slope is rise / run, and a horizontal line has a slope of 0 (0 rise over infinite run), finding the slope at the function’s minimum can be thought of as “descending” to a gradient of zero. The name gradient descent makes sense, because once the gradient decent algorithm has run for a given function, the computed parameters will descend the slope at the computed point to zero.&lt;/p&gt;

&lt;p&gt;Providing clear examples of the &lt;em&gt;how&lt;/em&gt; of ML are among the reasons I’m writing this &lt;a href=&quot;http://benbrostoff.github.io/2017/09/19/why-ml/&quot;&gt;blog series&lt;/a&gt;, so let’s use gradient descent on the following function:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(x - 5) ** 2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I write this function in this form as opposed to &lt;code&gt;x ** 2 - 10x + 25&lt;/code&gt; because it’s easy to see that the function is minimized at 5. Gradient descent should return &lt;code&gt;5&lt;/code&gt; for &lt;code&gt;x&lt;/code&gt; if the algorithm lives up to its name.&lt;/p&gt;

&lt;p&gt;The gradient descent algorithm itself is all of one line, but importantly requires 1) the derivative (also synonymous with slope) of the function and 2) a small number to descend the gradient. The small number - which we’ll set to &lt;code&gt;0.01&lt;/code&gt; in this example - is necessary because it allows the gradient to move on each iteration of the algorithm. The derivative can be &lt;a href=&quot;https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules/ab-diff-negative-fraction-powers/a/power-rule-review&quot;&gt;computed through the power rule&lt;/a&gt;, which I had to review via the provided link from Khan academy. In short, moving the exponent &lt;code&gt;2&lt;/code&gt; down and raising the result to &lt;code&gt;2-1&lt;/code&gt; returns a derivative function of &lt;code&gt;2(x-5) ** 1&lt;/code&gt; or &lt;code&gt;2x - 10&lt;/code&gt;. We now have the parameters necessary to run gradient descent for a few iterations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/first_iterations.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What this algorithm does is 1) take the previous minimized value (or start at zero) and 2) subtract the slope function with the previous minimum value passed in, multiplied by the small number.&lt;/p&gt;

&lt;p&gt;And if run for 10,000 iterations or even 100,000 iterations, with &lt;code&gt;0.01&lt;/code&gt; as the small number, &lt;code&gt;x&lt;/code&gt; converges to &lt;code&gt;5&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/converge.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what if instead of a small number we used a huge number? What is clear from experimentation is that numbers larger than one will never allow the solution to converge to &lt;code&gt;5&lt;/code&gt;. While a small number of &lt;code&gt;0.99&lt;/code&gt; converges to &lt;code&gt;5&lt;/code&gt;, a small number of &lt;code&gt;1.0&lt;/code&gt; bounces the solution back and forth between &lt;code&gt;10&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; forever. A couple of iterations in this sequence are instructive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/bad_learning_rate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The value selected for the small number is extremely important. Selection of the small number is an example of &lt;em&gt;hyperparamater tuning&lt;/em&gt; in machine learning, which we’ll explore in a later post.&lt;/p&gt;

&lt;p&gt;Please note all the code for this post can be found &lt;a href=&quot;https://github.com/BenBrostoff/ml-series-source&quot;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Does Gradient Descent Help Machines Learn?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The small number mentioned above is called the &lt;em&gt;learning rate&lt;/em&gt; in ML contexts, and drives the learning in machine learning. The function the gradient descent algorithm is applied on is the &lt;em&gt;loss function&lt;/em&gt;, where loss is the difference between the prediction made from a machine learning example and actual reality. For example, if an ML model predicted an NFL running back will finish with &lt;code&gt;17&lt;/code&gt; touch downs for the 2016 season and the reality is &lt;code&gt;11&lt;/code&gt;, the loss in this example is &lt;code&gt;6&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the next post, I’m excited to explore an actual application of gradient descent in machine learning.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/10/09/gradient-descent-intuition.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/10/09/gradient-descent-intuition.html</guid>
      </item>
    
      <item>
        <title>Why I'm Writing About ML (ML Series, Part 1)</title>
        <description>&lt;blockquote&gt;

  &lt;p&gt;At the heart of science is an essential balance between two seemingly contradictory attitudes - an openness to new ideas, no matter how bizarre or counterintuitive, and the most ruthlessly skeptical scrutiny of all ideas, old and new.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Carl Sagan, &lt;a href=&quot;https://www.amazon.com/Demon-Haunted-World-Science-Candle-Dark/dp/0345409469&quot;&gt;The Demon-Haunted World&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;I know of no better tool than writing to explore my own understanding of technical subjects. After years of reading about machine learning in the press, programmer blogs and from technology leaders of our time, I want to investigate ML on my own terms. This investigation may take months or years or, based on how quickly the field is changing, forever - I’m not sure. What I can say is that my goal in writing a multi-part series is no less than what Sagan describes in the above quote. I intend to be both open to extraordinary claims about machine learning and simultaneously skeptical.&lt;/p&gt;

&lt;p&gt;The value I believe I can add in writing this series is in infusing each post with as much illustrative code as possible. I want to explore not only programming libraries, but the cloud services that allow library code to be run with GPU enabled computing power. While I have been extraordinarily influenced by previous explorations of ML like Tim Urban’s &lt;a href=&quot;https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html&quot;&gt;AI Revolution Series&lt;/a&gt;, these posts are going to be technical and experiment heavy.&lt;/p&gt;

&lt;p&gt;I want to answer for myself not only &lt;em&gt;why&lt;/em&gt; machine learning works, but also the various &lt;em&gt;hows&lt;/em&gt;. I will try to approach as much as I can from first principles in answering &lt;em&gt;why&lt;/em&gt; machine learning is effective. Where ML relies on ideas from linear algebra and calculus, I promise to explore these fields as well. On the subject of &lt;em&gt;how&lt;/em&gt; machine learning works today, I want to start first with as few libraries as possible, and then move slowly into popular libraries like &lt;code&gt;TensorFlow&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What these posts will &lt;em&gt;not&lt;/em&gt; do is engage in futurism discussions or projections of what machine learning could do. I am going to stay focused on what machine learning &lt;em&gt;can do at present&lt;/em&gt;. Any demonstrations of machine learning capabilities I’ll post on GitHub. As someone with no formal ML background, I encourage anyone (un)lucky enough to be reading these posts to raise issues and provide guidance on what I could be doing better.&lt;/p&gt;

&lt;p&gt;Along the way, I’ll be diving into the following resources. I’ll add to this list as this series takes shape:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Andrew Ng’s deep neural network &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Coursera specialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sebastian Raschka’s &lt;a href=&quot;https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0&quot;&gt;Python Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, I want to manage expectations about how often I’ll be posting in this series and post lengths. &lt;a href=&quot;https://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692&quot;&gt;Deep Work&lt;/a&gt; is one of my favorite books, and in the spirit of Cal Newport’s suggestion to budget deep work hours to projects, I intend to devote 3-5 hours of deep work per week to this project. In terms of length, these posts might be short or long depending on the idea being explored. I have set a short-term goal of posting once per week for the next 5 weeks; after that, I’ll evaluate the quality of those posts and make improvements for the next ones.&lt;/p&gt;

&lt;p&gt;I’m excited to get started. If these posts can change my own opinion on machine learning, than they were worth doing. If anyone else reads them, it’s an added bonus.&lt;/p&gt;

&lt;p&gt;Here’s to openness and ruthless skepticism.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/09/19/why-ml.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/09/19/why-ml.html</guid>
      </item>
    
      <item>
        <title>RTD</title>
        <description>&lt;p&gt;RTD - Read the Docs. That phrase cuts across industries - I have heard it just as much as a programmer as I did as an investment banker. And while when said verbally, RTD (sometimes RTFD) usually lands with a hint of annoyance, I really believe it’s some of the most important career advice I have ever received.&lt;/p&gt;

&lt;p&gt;I recently was writing some unit tests to check that a small wrapper around the &lt;a href=&quot;https://aws.amazon.com/documentation/ses/&quot;&gt;AWS.SES API&lt;/a&gt; was invoked correctly and logging error messages as expected. Using &lt;a href=&quot;https://github.com/sinonjs/sinon&quot;&gt;&lt;code&gt;sinon&lt;/code&gt;&lt;/a&gt; and the &lt;a href=&quot;https://github.com/dwyl/aws-sdk-mock&quot;&gt;&lt;code&gt;aws-sdk-mock&lt;/code&gt;&lt;/a&gt; library, I had some code that looked like the below to check error handling:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/1cba16104882f0c73609ff105ee39d5d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;As background, &lt;code&gt;mock&lt;/code&gt; from &lt;code&gt;aws-sdk-mock&lt;/code&gt; takes three arguments, the third of which is a function that, in the case of mocking &lt;a href=&quot;http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SES.html#sendEmail-property&quot;&gt;sendEmail&lt;/a&gt;, takes the email params and a callback. The callback itself takes two arguments - an error message and data from the response.&lt;/p&gt;

&lt;p&gt;I received a comment from my boss on this code review to the effect that every time I was using &lt;code&gt;callsFake&lt;/code&gt;, I could just be using &lt;a href=&quot;http://sinonjs.org/releases/v3.0.0/stubs/&quot;&gt;&lt;code&gt;callsArgWith&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;http://sinonjs.org/releases/v3.0.0/stubs/&quot;&gt;documentation&lt;/a&gt; for &lt;code&gt;callsArgWith&lt;/code&gt; points to &lt;code&gt;callsArg&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;stub.callsArgWith(index, arg1, arg2, …)&lt;/p&gt;

  &lt;p&gt;Like callsArg, but with arguments to pass to the callback.&lt;/p&gt;

  &lt;p&gt;stub.callsArg(index)&lt;/p&gt;

  &lt;p&gt;Causes the stub to call the argument at the provided index as a callback function. stub.callsArg(0); causes the stub to call the first argument as a callback.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;It made sense - my only goal was to fake invoking the callback &lt;code&gt;sendEmail&lt;/code&gt; takes following resolution of the email &lt;code&gt;Promise&lt;/code&gt;. Giving it an error didn’t require passing in a new function to &lt;code&gt;callsFake&lt;/code&gt; - &lt;code&gt;sinon&lt;/code&gt; already offers this functionality:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sendEmailStub = sandbox.stub().callsArgWith(1, new Error());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And because you’re always one click a way in your IDE from &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/lib/sinon/default-behaviors.js#L55&quot;&gt;seeing the source&lt;/a&gt; in &lt;code&gt;node_modules&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/14244abba5cb9ee47dac6884bc862abf.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;after which there’s an easy path to cloning the source repo and after some investigation finding your way to &lt;code&gt;callCallback&lt;/code&gt; in &lt;code&gt;lib/sinon/behavior.js&lt;/code&gt;, which gets the callback using &lt;code&gt;callArgAt&lt;/code&gt; set in the source we looked at, which then takes the &lt;code&gt;callbackArguments&lt;/code&gt; sans the &lt;code&gt;fake&lt;/code&gt; and &lt;code&gt;position&lt;/code&gt; arguments passed in &lt;code&gt;callsArgWith&lt;/code&gt; (note the function signature in the docs differs from what is the source - this is because &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/lib/sinon/behavior.js#L204&quot;&gt;&lt;code&gt;addBehavior&lt;/code&gt;&lt;/a&gt; will always add functions to the stub prototype with the first argument being the stub itself).  &lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/test/stub-test.js#L574&quot;&gt;here’s the test in &lt;code&gt;sinon&lt;/code&gt;&lt;/a&gt;, clearly demonstrating that a callback passed as the second argument to a stub should be able to be passed any argument with &lt;code&gt;callsArgWith&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/5951ea3710240f413739438620148022.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In looking at the &lt;code&gt;sinon&lt;/code&gt; docs and then the Amazon SES docs, I realized there was probably an ocean of API functionality I was not taking advantage of. And the only way to find out would be to study and experiment. Which is the fun of being an engineer.&lt;/p&gt;

&lt;p&gt;I bring up this example because I think it illustrates the importance of reading documentation (and even better, source code). I was familiar with &lt;code&gt;callsFake&lt;/code&gt;, so every testing situation looked like a nail to its hammer. This is what happens when API docs are read quickly and with the intent of completing a task. The cost is that robust APIs are used incorrectly, and code that should be short and sweet becomes long and ugly.&lt;/p&gt;

&lt;p&gt;I know this seems obvious, but I wanted to write about it because I know there are different gradients of RTD. In banking, you might read the earnings call transcript but not the 10-Q; only the part of the S-1 with the risks to the business; the first tab of the huge Excel model.&lt;/p&gt;

&lt;p&gt;In programming, there’s cherry-picking documentation; reading the documentation without the source code; starting a project with “getting started” documentation and never revisiting the docs again, and so much more.&lt;/p&gt;

&lt;p&gt;A lot of this can be argued against with allusions to time management principles. How am I supposed to get anything done if I’m reading forty thousands pages of documentation and source code? Obviously, reading all the documentation - especially for something like the AWS SDK - is impossible. But transferring a 3 hour chunk of time from 70 / 30 writing code / reading docs to 65 / 35 writing code / reading docs?&lt;/p&gt;

&lt;p&gt;You might end up improving your code and reading some interesting source code from a great library  &lt;/p&gt;
</description>
        <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/08/05/rtd.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/08/05/rtd.html</guid>
      </item>
    
      <item>
        <title>Upgrades</title>
        <description>&lt;p&gt;I bought a new MacBook Pro 2016 today, which represents a large upgrade for me. I was on an MBP 2012 before. The performance improvements are remarkable. Just using it today felt like one of those (ever more frequent) times when technology shocks you with how quickly it improves. I got to thinking about how many open source tools just in the last few years have sped up my workflow as a developer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;Anaconda&lt;/li&gt;
  &lt;li&gt;Jupyter notebooks&lt;/li&gt;
  &lt;li&gt;ES6 (especially &lt;code&gt;async / await&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;yarn&lt;/li&gt;
  &lt;li&gt;React and React Native&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the last decade (almost half decade), at some point one of these things did not exist. I could not envision myself living without any one of them now.&lt;/p&gt;

&lt;p&gt;As someone who uses a lot of open source tools, I feel very privileged. I am the beneficiary of frequent upgrades. When I need to debug someone else’s library, it’s generally not too terrible, and almost half the time there’s a robust conversation on GitHub about the exact issue I’m having. The other half of the time, I’m probably using the API incorrectly. Being a developer is fun because nothing stays broken for that long.&lt;/p&gt;

&lt;p&gt;But yes, more things do break, and complexity increases. That’s a given. And Hacker News, reddit and Twitter will remind you ad nauseam that everything is terrible and you’re part of the problem.&lt;/p&gt;

&lt;p&gt;I sometimes worry that these channels have become so vocal that they’re discouraging would-be makers from contributing. &lt;a href=&quot;https://www.kennethreitz.org/essays/the-reality-of-developer-burnout&quot;&gt;Kenneth Reitz wrote recently about publish-only mode&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;I didn’t want to lose what I valued most about my position within our community — being able to influence the world I cared so much about. So, I unfollowed everyone on Twitter. Every single person. I stopped paying attention to tech trends and reading hacker news. I went into publish-only mode.&lt;/p&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;I continue to find these words inspirational, because I think they’re more relevant than ever. The open source ecosystem is meant to be lived in and improved. To the extent that developers can adopt improved software with as little friction as possible, I believe the world becomes a better place.&lt;/p&gt;

&lt;p&gt;I was reminded today that software and hardware improvements can really impact human life in a positive way. I want to help bring upgrades into the open source ecosystem as much as I can. And the road to upgrades lies in &lt;a href=&quot;https://www.amazon.com/dp/B00X47ZVXM/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1&quot;&gt;deep work&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, here’s to publish-only mode, and resolving to be a part of the next decade of upgrades.  &lt;/p&gt;
</description>
        <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/05/06/upgrades.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/05/06/upgrades.html</guid>
      </item>
    
      <item>
        <title>Destructuring Assignment</title>
        <description>&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;The foundation of the modern world is developer tools.&lt;/p&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/mxcl/status/619373095199969280?lang=en&quot;&gt;Max Howell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think destructuring assignment is one of the most useful tools in the ES6 toolkit. &lt;/p&gt;

&lt;p&gt;First off, a definition of destructuring assignment from the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment&quot;&gt;MDN docs&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;The destructuring assignment syntax is a JavaScript expression that makes it possible to extract data from arrays or objects into distinct variables.&lt;/p&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Said another way, destructuring allows for creation or assignment of variables from parts of arrays or objects. Because it is not the job of developer documentation to sell different features of the API, or promote common use cases (depending on your perspective), I want to take the time to do it here. I find myself using destructuring assignment time and time again for a host of different reasons. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Imports, Function Arguments and Variables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This one I think is the most straightforward use case suggested by the docs. Extracting functionality from large Node modules, pulling out object properties from an argument in a function parameter, or creating multiple &lt;code&gt;const&lt;/code&gt;s from an object are all low hanging fruit. What gets interesting is when objects have deeply nested structures and can be destructured. Consider this ridiculous example:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/c30b2ef1f5c4022c6b977ef75c1ac590.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;While I think more nesting levels of destructuring obfuscate code, this technique is really useful when taken with a grain of salt. This usefulness especially shines when using &lt;code&gt;Array&lt;/code&gt; functions like &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt;. Consider the scenario where you get a bunch of database results that have a bunch of properties, some of which have values of type &lt;code&gt;Array&lt;/code&gt;. Maybe the client needs to show all the results but also some aggregations (i.e. for whatever reason, the aggregation occurs outside of the DB).&lt;/p&gt;

&lt;p&gt;A data request like “show me total points scored in the first half by this subset of basketball players” works well with destructuring:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/ae147b4908812281305a40a760f76aaf.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I particularly like how variable naming (&lt;code&gt;firstQ&lt;/code&gt;, &lt;code&gt;secondQ&lt;/code&gt;) can take place in the second function argument of the &lt;code&gt;reduce&lt;/code&gt; function as opposed to in the function itself.&lt;/p&gt;

&lt;p&gt;Speaking of renaming things…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Renaming Things&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because context so often determines what the purpose of a piece of data is, it makes sense to me to rename the same piece of data depending on context.&lt;/p&gt;

&lt;p&gt;For instance, let’s assume you had some player data from the NBA that looked like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/2604218271ca8d8b60577e76be6f8735.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;You then have one function that simply takes in the data for one player and returns their points per game.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/041e34686f448133840c6bc94b95c62f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;It makes sense to call the argument here &lt;code&gt;pointsPerGame&lt;/code&gt;, as the purpose of this function is simply to get that value. &lt;/p&gt;

&lt;p&gt;In contrast, let’s say you have a function that takes two arguments - data for an individual player and all player data - and then returns the data for all players with more PPG than that player. Here, changing the name &lt;code&gt;pointsPerGame&lt;/code&gt; to &lt;code&gt;ppgThreshold&lt;/code&gt; makes a lot of sense.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/0f2e87a7a72043c17183035ec865272e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;We can actually go one step further here and destructure and rename the argument in the &lt;code&gt;filter&lt;/code&gt; function for clarity, using &lt;code&gt;playerPpg&lt;/code&gt; instead of &lt;code&gt;player.pointsPerGame&lt;/code&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/e0b5b1a6c94f58e5a26ae74c33688c89.js&quot;&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Applying Left-to-Right Thinking to Right-To-Left&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing in particular I like about destructuring assignment is that &lt;em&gt;it makes your brain reverse how you think about variable assignment&lt;/em&gt;. Pre ES6 destructuring, assigning a variable to the second and third elements of a Javascript array looked something like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/d09ea100babb266b1c8b918348faf534.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The natural thought process at least for me here is “look to the right hand side for meaning”. I see the numbers array and the indices at 2 and 3, and then understand.&lt;/p&gt;

&lt;p&gt;With destructuring, the above can be done as follows:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/600e5c1e6b3e33a420829b3bb3062fd6.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Now, my thought process becomes “first, look to the left hand side for meaning”. I see the square brackets around &lt;code&gt;a&lt;/code&gt; and understand I need to get the first element of the right hand side. A side benefit is I immediately get that &lt;code&gt;numbers&lt;/code&gt; is an &lt;code&gt;Array&lt;/code&gt; with at least three elements too (obviously not guaranteed, but we know that’s the intent at least), so destructuring can convey type information too.&lt;/p&gt;

&lt;p&gt;As a side note here, Python gets close to allowing you to do this with lists, but will throw a &lt;code&gt;ValueError: too many values to unpack&lt;/code&gt; if the size of the list does not match the number of variables. &lt;/p&gt;

&lt;p&gt;Overall, I’m amazed by how many different use cases ES6’s destructuring syntax has (and I’m sure there’s more to come in ES7). This feature is the rare tool that’s multi-purpose and easy to get the hang of. Moreover, it forces me to think about programming in a different way - looking to the left hand side of a variable assignment for meaning.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/01/02/destructuring-assignment.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/01/02/destructuring-assignment.html</guid>
      </item>
    
      <item>
        <title>Endless Learning</title>
        <description>&lt;p&gt;There is an infinite amount of knowledge to consume and an average human only lives &lt;a href=&quot;http://www.who.int/gho/mortality_burden_disease/life_tables/situation_trends/en/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;71.4 years&lt;/strong&gt;&lt;/a&gt;. I wanted to write this post to remind myself of 1) why the current landscape for learning is so excellent and 2) strategies that work for mining this landscape for its excellence.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;An Excellent Landscape&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are lots of fantastic, thoughtful writers. The amount of superb programming blogs in particular impresses me – &lt;a href=&quot;http://jvns.ca/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Julia Evans&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;http://geoff.greer.fm/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Geoff Greer&lt;/strong&gt;&lt;/a&gt; are at present my two favorites. &lt;a href=&quot;http://waitbutwhy.com/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Wait But Why&lt;/strong&gt;&lt;/a&gt; by Tim Urban is basically free books on interesting topics (thanks to my friend Max for showing me this awesome &lt;a href=&quot;http://waitbutwhy.com/2015/05/elon-musk-the-worlds-raddest-man.html&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;4 part WBW series on Elon Musk&lt;/strong&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The podcasting landscape – since I started listening to podcasts – has never been better. &lt;a href=&quot;http://fourhourworkweek.com/podcast/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Tim Ferriss&lt;/strong&gt;&lt;/a&gt; lands superb guest after superb guest. &lt;a href=&quot;http://www.stuffyoushouldknow.com/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Stuff You Should Know&lt;/strong&gt;&lt;/a&gt; is an hour learning about brand new stuff. And I’ll always listen to &lt;a href=&quot;https://soundcloud.com/the-bill-simmons-podcast&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Bill Simmons&lt;/strong&gt;&lt;/a&gt; wax poetic on basketball for an hour.&lt;/p&gt;

&lt;p&gt;Netflix promises whatever thing you happen to be interested in. I just watched &lt;a href=&quot;http://www.bbc.co.uk/programmes/b0074t4x&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Planet Earth: Seasonal Forests&lt;/strong&gt;&lt;/a&gt; and learned more about the biosphere than at any point of being an adult.&lt;/p&gt;

&lt;p&gt;I have been consuming these great things in part by abandoning my “traditional” media sources. In high school and college I had various mentors tell me to read the Wall Street Journal religiously – I admit I don’t do that any more.  While that seemed like sound advice at the time, I feel I am learning more by foregoing newspapers. WSJ, NYT and the Boston Globe – the three newspapers that I see lying around most often – seem to have become obsessed with gossipy political stories.&lt;/p&gt;

&lt;p&gt;Books are always what I learn the most from.  Blogs and podcasts lead to strong book recommendations. Greer’s &lt;a href=&quot;http://geoff.greer.fm/2016/01/04/on-learning-c-part-1-k-r/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;four part series on learning C&lt;/strong&gt;&lt;/a&gt; led me to purchase &lt;a href=&quot;http://www.amazon.com/Programming-Language-Brian-W-Kernighan/dp/0131103628/ref=pd_sim_14_1?ie=UTF8&amp;amp;dpID=41qX6YdIJ7L&amp;amp;dpSrc=sims&amp;amp;preST=_AC_UL320_SR244%2C320_&amp;amp;refRID=1B6FHHVSB9JKW4TVBCMG&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;K&amp;amp;R&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;http://www.amazon.com/Computer-Systems-Programmers-Perspective-Edition/dp/0136108040&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Computer Systems: A Programmer’s Perspective&lt;/strong&gt;&lt;/a&gt;. Ferriss’s podcast has been responsible for me reading the following books: &lt;a href=&quot;http://www.amazon.com/Not-Fade-Away-Short-Lived/dp/006073731X&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Not Fade Away&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;http://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/006124189X&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Influence&lt;/strong&gt;&lt;/a&gt;, the &lt;a href=&quot;http://www.amazon.com/Art-Learning-Journey-Optimal-Performance/dp/0743277465&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Art of Learning&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;http://www.amazon.com/War-Art-Winning-Creative-Battle/dp/1501260626&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The War of Art&lt;/strong&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;I’ve been lucky to have fantastic friends who led me to read books that changed my life. &lt;/p&gt;

&lt;p&gt;Chris – thank you for Tony Bramwell’s &lt;a href=&quot;http://www.amazon.com/Magical-Mystery-Tours-Life-Beatles/dp/0312330448&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Magical Mystery Tours&lt;/strong&gt;&lt;/a&gt;. I still have your copy. &lt;/p&gt;

&lt;p&gt;Joseph – a big tip of the hat for &lt;a href=&quot;http://www.amazon.com/Obstacle-Way-Timeless-Turning-Triumph/dp/1591846358&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The Obstacle is the Way&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;http://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Deep Work&lt;/strong&gt;&lt;/a&gt;. I still travel with &lt;a href=&quot;http://www.amazon.com/Meditations-Thrift-Editions-Marcus-Aurelius/dp/048629823X&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Meditations&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dad – thank you for &lt;a href=&quot;http://www.amazon.com/Values-Game-Bill-Bradley/dp/0767904494&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Values of the Game&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Being Selective&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There is an endless amount of learning to be done, and no better time to do it. I understand the temptation to endlessly bemoan targeted ads and sponsored content and promotional tweets. However, I really believe these things can be easily avoided. For me, here are the strategies that have worked:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Two books per month, at the same time&lt;/em&gt; - I listed this one first because it has easily been the most useful. Since 2014, I have been trying to read at least 20 books per year - you can see &lt;a href=&quot;http://benbrostoff.github.io/books/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;my progress here&lt;/strong&gt;&lt;/a&gt;. The two per month will get me a little beyond this goal. I try my best to make these two books extremely different so switching between them is interesting. For instance, right now I’m reading Neal Stephenson’s &lt;a href=&quot;https://www.amazon.com/Snow-Crash-Neal-Stephenson-ebook/dp/B000FBJCJE?ie=UTF8&amp;amp;qid=&amp;amp;ref_=tmm_kin_swatch_0&amp;amp;sr=&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Snow Crash&lt;/strong&gt;&lt;/a&gt; and Doris Kearns Goodwin’s &lt;a href=&quot;http://www.amazon.com/Team-Rivals-Political-Abraham-Lincoln/dp/0743270754&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Team of Rivals&lt;/strong&gt;&lt;/a&gt;. The former is a sci-fi thriller that takes place partly in reality and partly in the metaverse, a VR centered manifestation of reality. The characters use a funny, futuristic slang and there’s lots of violence, gore and riffs on computers. The latter (disclaimer: I’ve been reading this on and off since February in part because it’s 944 pages) follows Lincoln’s rise to the presidency through his death, while also simultaneously following the members of his war cabinet - William H. Seward, Salmon P. Chase and Edward Bates. &lt;em&gt;Rivals&lt;/em&gt; in both chronology and tone is the opposite of &lt;em&gt;Snow Crash&lt;/em&gt;, which makes reading them at the same time awesome. And having both on Kindle makes it easy to switch from one to the other.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Twitter selectively&lt;/em&gt; - I can’t think of many bigger timesinks than scrolling through Twitter. That said, a lot of people I follow from time to time tweet out high quality reads I never would have found myself. For every one of these tweets, maybe five or six occur in between which are less interesting, or require context to even understand (which then requires more scrolling through Twitter). &lt;a href=&quot;https://twitter.com/pmarca&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Marc Andreessen&lt;/strong&gt;&lt;/a&gt; is perhaps the biggest example of this phenomenon. A lot of tools exist for parsing out the good tweets. I built one for myself because I wanted to use Go - &lt;a href=&quot;https://github.com/BenBrostoff/twitter-news&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;my little JSON API&lt;/strong&gt;&lt;/a&gt; finds the top tweets from a user’s last 200 from a couple people I find particularly interesting. &lt;a href=&quot;http://nuzzel.com/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Nuzzel&lt;/strong&gt;&lt;/a&gt; is the other solution I use, which basically highlights links that multiple people I follow tweet.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Pocket and Rex&lt;/em&gt; - I’ve been using &lt;a href=&quot;https://getpocket.com/a/queue/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Pocket&lt;/strong&gt;&lt;/a&gt; for two years and &lt;a href=&quot;https://itunes.apple.com/us/app/rex-share-recommendations/id965827767?mt=8&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Rex&lt;/strong&gt;&lt;/a&gt; for two months. My Pocket strategy is simple but works well for me - during the weekdays, I pocket a bunch of stuff while I eat lunch at my computer and then read it on Saturday / Sunday mornings. As an aside, Pocket is also good for those times when you come across a Stack Overflow post not really directly related to the thing you’re working on, but nonetheless interesting. If a friend e-mails or texts me a link, I’ll pocket it immediately. Rex, still in its relative infancy as an app, is good because users can only recommend stuff. Said another way, there’s no outlet for debates or vitriol or trolling like on Twitter. As Rex’s CEO, Chris Smith, helpfully pointed out to me, &lt;a href=&quot;https://twitter.com/bmb21/status/731612756843991040&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Rex links are Pocketable, and it may have a public API in the future&lt;/strong&gt;&lt;/a&gt;, which would allow for some awesome customization possibilities.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Making some time for daily news&lt;/em&gt; - I enjoy following sports and the stock market on a regular basis, as well as keeping tabs on tech and foreign affairs news (really, anything that is not the 2016 presidential race). I find headline scanning on ESPN, Bloomberg and Hacker News is generally sufficient to accomplish this task, where I’ll pocket the longer reads if they seem good. I have little cron jobs that allow me to keep up with my &lt;a href=&quot;https://github.com/BenBrostoff/Fantasy-Baseball-Scraper&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;fantasy baseball team&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/BenBrostoff/daily_stock_report&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;stock portfolio&lt;/strong&gt;&lt;/a&gt;. Stuff like this is a good excuse to do some programming with new tools and languages in a no pressure context.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Conclusion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Life is short, and reading material is long. I feel a lot more satisfied with my free time since I abandoned newspapers and focused exclusively on good blogs, books and podcasts. Being disciplined with the books I read each month, using Twitter in a highly selective way, and combining Pocket and Rex functionality helps me discover good material. The daily news I’m interested in I can get through headline scanning and automated web scraping scripts.&lt;/p&gt;

</description>
        <pubDate>Sat, 28 May 2016 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2016/05/28/endless-learning.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2016/05/28/endless-learning.html</guid>
      </item>
    
      <item>
        <title>Devotion to Craft</title>
        <description>&lt;p&gt;A couple of my favorite things: the documentary &lt;a href=&quot;http://www.rottentomatoes.com/m/meru_2015/&quot; target=&quot;_blank&quot;&gt;Meru&lt;/a&gt;, the essay &lt;a href=&quot;http://www.newyorker.com/magazine/2014/04/21/in-deep-2&quot; target=&quot;_blank&quot;&gt;In Deep&lt;/a&gt;, the 30 for 30s on &lt;a href=&quot;http://espn.go.com/30for30/film?page=into-the-wind&quot; target=&quot;_blank&quot;&gt;Terry Fox&lt;/a&gt; and &lt;a href=&quot;http://espn.go.com/30for30/film?page=the-birth-of-big-air&quot; target=&quot;_blank&quot;&gt;Dave Mirra&lt;/a&gt;. In all of them, there are people who in the truest sense of the words are &lt;em&gt;willing to risk life and limb for their craft&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;That is exceedingly rare in a world where we have all the things we need and more.&lt;/p&gt;
</description>
        <pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2016/05/08/caring-about-craft.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2016/05/08/caring-about-craft.html</guid>
      </item>
    
  </channel>
</rss>