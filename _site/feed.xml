<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ben Brostoff</title>
    <description></description>
    <link>http://benbrostoff.github.io/</link>
    <atom:link href="http://benbrostoff.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Redux - .subscribe</title>
        <description>&lt;p&gt;My &lt;a href=&quot;http://benbrostoff.github.io/2018/04/09/redux-dispatch-and-subscribe.html&quot;&gt;last post&lt;/a&gt; explored calling &lt;code&gt;.dispatch&lt;/code&gt; on a Redux store without listeners; in this post, I will add listeners to the toy application we’ve been building in this series and trace the &lt;code&gt;subscribe&lt;/code&gt; source in the process. I’m now using &lt;code&gt;v4.0.0&lt;/code&gt;, which was &lt;a href=&quot;https://github.com/reactjs/redux/releases/tag/v4.0.0&quot;&gt;released on April 16th&lt;/a&gt; - when I started on this series, Redux was on &lt;code&gt;v3.7.2&lt;/code&gt;. To begin our code exploration, I’ll call subscribe on our Redux store and pass in a callback that just logs the state of the store to the console:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/b296bda2b96040d0ec9950a7402d2fe6.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Let’s begin with the documentation for &lt;code&gt;subscribe&lt;/code&gt; in the source.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Adds a change listener. It will be called any time an action is dispatched,&lt;br /&gt;
and some part of the state tree may potentially have changed. You may then&lt;br /&gt;
call &lt;code&gt;getState()&lt;/code&gt; to read the current state tree inside the callback.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The next items in the docs before the function signature are about calling &lt;code&gt;dispatch&lt;/code&gt; from a change listener, which we’ll ignore for now since our toy example does not do this. The function signature explains that &lt;code&gt;subscribe&lt;/code&gt; expects a function that will be invoked on every dispatch and returns a function that when invoked will unsubscribe the listener:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;@param {Function} listener A callback to be invoked on every dispatch.&lt;br /&gt;
@returns {Function} A function to remove this change listener.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The full &lt;code&gt;v4.0.0&lt;/code&gt; source  for &lt;code&gt;subscribe&lt;/code&gt; is below. When reading this blog post, it may be useful to split screen and have it open, although I’ll add gists where relevant:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/226d5e7ea1c4ebe1c8b8293ea29ee795.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;As we’ve seen elsewhere in the Redux source, the beginning of the function body is type checking and raising errors if the expectations outlined in the docs are not met. Redux checks to see that &lt;code&gt;listener&lt;/code&gt; is a function and that a &lt;code&gt;store.dispatch&lt;/code&gt; call is not in progress when &lt;code&gt;subscribe&lt;/code&gt; is invoked.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/ff4a896323dd937155bb18766278a7bc.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Two notable things then happen:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/07b04cfde6c65d1cf6da4e09b7280e40.js&quot;&gt;&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Redux sets a variable that will be updated later called &lt;code&gt;isSubscribed&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; - this makes sense from a naming perspective, as we’re subscribing to a function. We’ll come back to this variable when discussing &lt;code&gt;unsubscribe&lt;/code&gt;, which predictably sets it to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A function called &lt;code&gt;ensureCanMutateNextListeners&lt;/code&gt; is called next, which is small despite the long name. All this does is check if two variables declared in &lt;code&gt;createStore&lt;/code&gt; are the same array (and originally they are - &lt;code&gt;let nextListeners = currentListeners&lt;/code&gt; happens on &lt;code&gt;createStore&lt;/code&gt;). If they are, &lt;code&gt;nextListeners&lt;/code&gt; is set equal to a copy of &lt;code&gt;currentListeners&lt;/code&gt; via &lt;code&gt;.slice()&lt;/code&gt;, thereby destroying the equality:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/994b547071145785093b0f1970485df6.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;code&gt;listener&lt;/code&gt; passed to &lt;code&gt;subscribe&lt;/code&gt; is pushed into the array of &lt;code&gt;nextListeners&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that the Redux docs have numerous references to the term “snapshotting listeners”, and I take that to mean the function &lt;code&gt;ensureCanMutateNextListeners&lt;/code&gt; is serving. The Redux source never adds or removes a listener without first copying &lt;code&gt;currentListeners&lt;/code&gt; if &lt;code&gt;nextListeners&lt;/code&gt; and &lt;code&gt;currentListeners&lt;/code&gt; are a reference to the same value (&lt;code&gt;currentListeners&lt;/code&gt; is assigned to &lt;code&gt;nextListeners&lt;/code&gt; in &lt;code&gt;dispatch&lt;/code&gt; and in &lt;code&gt;createStore&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Finally, an &lt;code&gt;unsubscribe&lt;/code&gt; function is returned, which includes closures from variables from &lt;code&gt;subscribe&lt;/code&gt; and &lt;code&gt;createStore&lt;/code&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/6bba11e54f70eb88f5ee88772a8de3ae.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The first conditional and empty return is to ensure that calls to &lt;code&gt;unsubscribe&lt;/code&gt; after the first call do nothing. On the first &lt;code&gt;unsubscribe&lt;/code&gt; call, &lt;code&gt;isSubscribed&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt;; afterwards, there is no way to set it back to &lt;code&gt;true&lt;/code&gt;, since each &lt;code&gt;subscribe&lt;/code&gt; call creates a separate closure. Calls to &lt;code&gt;unsubscribe&lt;/code&gt; after the first one bail out as early as possible.&lt;/p&gt;

&lt;p&gt;Next, Redux again checks if a &lt;code&gt;dispatch&lt;/code&gt; call is in progress and throws an error if this is the case. The error here is to guard against calling unsubscribe while a reducer is executing. As an aside, this &lt;code&gt;if (isDispatching)&lt;/code&gt; and error throwing logic happens three times in the &lt;code&gt;createStore&lt;/code&gt; source - once in &lt;code&gt;dispatch&lt;/code&gt; (reducers cannot dispatch actions), once in &lt;code&gt;getState&lt;/code&gt; (cannot read state while reducer is executing) and once here.&lt;/p&gt;

&lt;p&gt;As alluded to earlier, Redux then sets &lt;code&gt;isSubscribed&lt;/code&gt; to false, guaranteeing future calls to &lt;code&gt;unsubscribe&lt;/code&gt; will do nothing.&lt;/p&gt;

&lt;p&gt;The meat of &lt;code&gt;unsubscribe&lt;/code&gt; is next:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/974773e6151b28f66e67395cad37abf5.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;We reviewed &lt;code&gt;ensureCanMutateNextListeners()&lt;/code&gt; - this call protects &lt;code&gt;nextListeners&lt;/code&gt; from being mutated by changes to &lt;code&gt;currentListeners&lt;/code&gt;. &lt;code&gt;nextListeners.indexOf(listener)&lt;/code&gt; gets the index in the array of listeners of the listener unsubscribe is tied to. Finally, &lt;code&gt;nextListeners.splice(index, 1)&lt;/code&gt; removes the listener from the &lt;code&gt;nextListeners&lt;/code&gt; array. An example may be helpful.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/32a397577c574c8509020709fc3a918a.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In the first unsubscribe call, &lt;code&gt;index&lt;/code&gt; is 2 (&lt;code&gt;funcC&lt;/code&gt; is the last listener in an array of three), then 1 (with &lt;code&gt;funcC&lt;/code&gt; removed, &lt;code&gt;funcB&lt;/code&gt; is now the last listener in an array of 2), then 0 (&lt;code&gt;funcA&lt;/code&gt; is the only listener in an array of one). &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/splice&quot;&gt;&lt;code&gt;splice&lt;/code&gt; is a mutative function&lt;/a&gt; that takes a position as the first argument and how many elements to delete as the second. With &lt;code&gt;unsubscribe&lt;/code&gt;, &lt;code&gt;splice&lt;/code&gt; is deleting the relevant listener at its respective position.&lt;/p&gt;

&lt;p&gt;Now that we’ve reviewed subscribe, we can actually trace what happens on a dispatch with a listener:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/d73da8dd6ad848a2ee5a575d97685b59.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;At the very end of &lt;code&gt;dispatch&lt;/code&gt; (the only remaining line is to return an action), Redux sets &lt;code&gt;currentListeners&lt;/code&gt; to &lt;code&gt;nextListeners&lt;/code&gt; and iterates through the current listeners in the order they were added. Each listener is then invoked with no arguments. It would be easy enough to give the listeners the current &lt;code&gt;action&lt;/code&gt;, but &lt;a href=&quot;https://github.com/reactjs/redux/issues/1057&quot;&gt;per the original author, Dan Abramov,&lt;/a&gt; this is a misuse of the library:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Subscribers should react to the new state, not to what happened.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In summary, the subscribe source:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Checks that the passed argument is a function and that the reducer passed to &lt;code&gt;createStore&lt;/code&gt; is not executing&lt;/li&gt;
  &lt;li&gt;Pushes the passed listener function to an array of listeners (&lt;code&gt;nextListeners&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Returns an unsubscribe function that can be invoked in order to remove the listener from the array of listener&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;dispatch&lt;/code&gt; then invokes every listener in &lt;code&gt;currentListeners&lt;/code&gt; (which is assigned to &lt;code&gt;nextListeners&lt;/code&gt;). Thus, any function passed to subscribe will be called on &lt;code&gt;dispatch&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That about wraps up the bulk of &lt;code&gt;createStore&lt;/code&gt;. Note that I skipped some of the API that was not part of Redux in its original 2015 state - &lt;code&gt;replaceReducer&lt;/code&gt; and Redux’s &lt;code&gt;observable&lt;/code&gt; functions were left out here.&lt;/p&gt;

&lt;p&gt;Next, I want to dive into &lt;code&gt;applyMiddleware&lt;/code&gt; and discussing how adding middleware to a Redux store works behind the scenes.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2018/04/20/redux-subscribe-listeners.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2018/04/20/redux-subscribe-listeners.html</guid>
      </item>
    
      <item>
        <title>Redux - .dispatch</title>
        <description>&lt;p&gt;Today’s post will look at what happens on &lt;code&gt;store.dispatch(action)&lt;/code&gt; calls after the &lt;code&gt;dispatch&lt;/code&gt; call invoked in &lt;code&gt;createStore&lt;/code&gt; (discussed in Part I of this Redux mini-series). As a refresher, the code below was generated in Part I. I’ll I’ve done is add a call to dispatch to increment my command line counter by 1.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/b52270ab73dd98be56e9a33747f60be9.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The counter now increments to the input number plus 1. So what happened here? &lt;code&gt;dispatch&lt;/code&gt; is actually a very short method (26 lines total, including white space), so this blog post will attempt to explain each line. The whole of dispatch is below (recall I’m using Redux &lt;code&gt;3.7.2&lt;/code&gt;):&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/08cb9127085c104a1c7cae02c77a7764.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I first want to review the commentary above the &lt;code&gt;dispatch&lt;/code&gt; method in the Redux source. Portions of the commentary deal with actions that are &lt;code&gt;Promise&lt;/code&gt;s, which Redux out of the box does not support. I’ll skip those sections for now. The first relevant section is below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dispatches an action. It is the only way to trigger a state change.&lt;/p&gt;

  &lt;p&gt;The &lt;code&gt;reducer&lt;/code&gt; function, used to create the store, will be called with the&lt;br /&gt;
current state tree and the given &lt;code&gt;action&lt;/code&gt;. Its return value will&lt;br /&gt;
be considered the &lt;strong&gt;next&lt;/strong&gt; state of the tree, and the change listeners&lt;br /&gt;
will be notified.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, invoking &lt;code&gt;dispatch&lt;/code&gt; is the only way to trigger a change in the store’s &lt;code&gt;currentState&lt;/code&gt;. &lt;code&gt;dispatch&lt;/code&gt; is invoked with an &lt;code&gt;action&lt;/code&gt; as an argument, which is passed to the reducer originally passed to &lt;code&gt;createStore&lt;/code&gt;. The return value from the reducer is the new &lt;code&gt;currentState&lt;/code&gt; of the tree. The final part of the sentence about change listeners is not yet relevant to the toy application I’m building, so I’ll ignore it for now.&lt;/p&gt;

&lt;p&gt;As previously noted, &lt;code&gt;dispatch&lt;/code&gt; takes an &lt;code&gt;action&lt;/code&gt; as an argument, which should be a POJO (helps with &lt;code&gt;redux-devtools&lt;/code&gt;), needs a type property and cannot be undefined.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;@param {Object} action A plain object representing “what changed”. It is a good idea to keep actions serializable so you can record and replay user sessions, or use the time travelling &lt;code&gt;redux-devtools&lt;/code&gt;. An action must have a &lt;code&gt;type&lt;/code&gt; property which may not be &lt;code&gt;undefined&lt;/code&gt;. It is a good idea to use string constants for action types.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The beginning of dispatch just covers these bases and raises errors where appropriate:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/6f4faaf4ebcaa0196d175ead31e42a76.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;There’s also a conditional that checks that &lt;code&gt;isDispatching&lt;/code&gt; is &lt;code&gt;truthy&lt;/code&gt;, and raises an error if it is.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/22ac068b76cf22c53e21355d75cdc366.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The reason for this error is to prevent calls to &lt;code&gt;dispatch&lt;/code&gt; from a &lt;code&gt;reducer&lt;/code&gt; (hence the error “Reducers may not dispatch actions.”). Raising this error can be done by passing the store into an action and calling &lt;code&gt;dispatch&lt;/code&gt; from the reducer, like so:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/0e64c47bcfa840ef244671432d60223e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The next part of &lt;code&gt;dispatch&lt;/code&gt; is a &lt;code&gt;try&lt;/code&gt; / &lt;code&gt;finally&lt;/code&gt; block that sets &lt;code&gt;isDispatching&lt;/code&gt; to true (a variable declared through &lt;code&gt;let&lt;/code&gt; in &lt;code&gt;createStore&lt;/code&gt;), sets &lt;code&gt;currentState&lt;/code&gt; to &lt;code&gt;currentReducer(currentState, action)&lt;/code&gt; (&lt;code&gt;currentState&lt;/code&gt; is also declared via &lt;code&gt;let&lt;/code&gt; in &lt;code&gt;createStore&lt;/code&gt;), and then in the &lt;code&gt;finally&lt;/code&gt; sets &lt;code&gt;isDispatching&lt;/code&gt; back to false. Again, the only usage of &lt;code&gt;isDispatching&lt;/code&gt; is in this function to prevent calling &lt;code&gt;dispatch&lt;/code&gt; in the reducer.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/c5f0b1a92590c4b306dbf7cdf282694e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Also, if you’re wondering why a &lt;code&gt;try&lt;/code&gt; / &lt;code&gt;finally&lt;/code&gt; here, the intent is to prevent Redux from never setting &lt;code&gt;isDispatching&lt;/code&gt; back to &lt;code&gt;false&lt;/code&gt;. This would prevent the reducer from ever firing again (because of the aforementioned “Reducers may not dispatch actions” error). I actually learned this from looking &lt;a href=&quot;https://github.com/reactjs/redux/pull/372&quot;&gt;at this Redux PR&lt;/a&gt;, and specifically this exchange:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/redux-series/why-try-finally.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final part of &lt;code&gt;dispatch&lt;/code&gt; before the return statement is to set a &lt;code&gt;listeners&lt;/code&gt; array equal to &lt;code&gt;currentListeners&lt;/code&gt;, which is then set to &lt;code&gt;nextListeners&lt;/code&gt;. Both the &lt;code&gt;*Listeners&lt;/code&gt; variables are declared via &lt;code&gt;let&lt;/code&gt; in &lt;code&gt;createStore&lt;/code&gt;. A &lt;code&gt;for&lt;/code&gt; loop then iterates through the listeners and invokes each one.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/323617a993a2bcfda4a8e77eff829b4f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In this toy example, both &lt;code&gt;currentListeners&lt;/code&gt; and &lt;code&gt;nextListeners&lt;/code&gt; are empty arrays, so there are no listeners to invoke.&lt;/p&gt;

&lt;p&gt;The return value is just the &lt;code&gt;action&lt;/code&gt; passed to it:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;@returns {Object} For convenience, the same action object you dispatched.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why this is convenient, I’m not yet sure, but will trust the source for now.&lt;/p&gt;

&lt;p&gt;We’ve done it! This post covers &lt;code&gt;dispatch&lt;/code&gt; without any listeners set. I’ll use the next post to generate listeners via &lt;code&gt;store.subscribe&lt;/code&gt; and analyze what happens in that listeners loop when listeners exist.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2018/04/09/redux-dispatch-and-subscribe.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2018/04/09/redux-dispatch-and-subscribe.html</guid>
      </item>
    
      <item>
        <title>Redux - createStore</title>
        <description>&lt;p&gt;I have been using React for about two years now, but have never used Redux for a major project. As I’m on vacation for a few days, I figure this is a great time to start learning Redux without the pressures of daily work. I want to take a different approach to learning frameworks this time around - using a bottom-up approach instead of top-down one. I’m going to review the source code (&lt;a href=&quot;https://github.com/reactjs/redux/tree/v3.7.2&quot;&gt;at tag &lt;code&gt;v3.7.2&lt;/code&gt;&lt;/a&gt;) instead of API docs.&lt;/p&gt;

&lt;p&gt;In my early experiments with Redux, I was delighted to find no frontend framework is necessary. Redux can be used in a Node project without issue, although that was obviously not the intent of Dan Abramov and its authors. Because experimenting outside of the browser leads to fewer distractions - no consideration of the DOM or browser APIs is necessary - this blog series will use a command-line app in Node as the project that invokes Redux.&lt;/p&gt;

&lt;p&gt;This blog post will focus on Redux’s &lt;code&gt;createStore&lt;/code&gt;, and use a dumb counter command line application to discuss the Redux store. &lt;code&gt;createStore&lt;/code&gt; only requires one argument (&lt;code&gt;reducer&lt;/code&gt;), and can optionally take a &lt;code&gt;preloadedState&lt;/code&gt; and &lt;code&gt;enhancer&lt;/code&gt;. Today, I’m only going to pass the required &lt;code&gt;reducer&lt;/code&gt; argument to &lt;code&gt;createStore&lt;/code&gt;. The Redux source code describes this argument as follows:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;@param {Function} reducer A function that returns the next state tree, given the current state tree and the action to handle.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;And what does &lt;code&gt;createStore&lt;/code&gt; return? &lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;Creates a Redux store that holds the state tree.&lt;br /&gt;
The only way to change the data in the store is to call &lt;code&gt;dispatch()&lt;/code&gt; on it.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Let’s trace the source code with only the &lt;code&gt;reducer&lt;/code&gt; argument. &lt;code&gt;createStore&lt;/code&gt; first checks that &lt;code&gt;reducer&lt;/code&gt; is a function. Not passing a &lt;code&gt;reducer&lt;/code&gt; that’s a function throws an error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&amp;gt; const { createStore } = require(&#39;redux&#39;);
&amp;gt; createStore();
Error: Expected the reducer to be a function.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That makes sense, as no function means no way to input a state tree and output a new state tree.&lt;/p&gt;

&lt;p&gt;The source then initializes 5 variables via &lt;code&gt;let&lt;/code&gt; - meaning these variables can be set to different values without error - and 6 functions. I plan to eventually review all of these in future posts, but for now I’ll simply discuss what happens when &lt;code&gt;createStore&lt;/code&gt; is invoked. After declaration of these variables and functions, a call to &lt;code&gt;dispatch&lt;/code&gt; is made (&lt;code&gt;dispatch({ type: ActionTypes.INIT })&lt;/code&gt;) before the function returns an object. The comment above the initial dispatch reads as follows:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;// When a store is created, an “INIT” action is dispatched so that every&lt;br /&gt;
// reducer returns their initial state. This effectively populates&lt;br /&gt;
// the initial state tree.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;ActionTypes.INIT&lt;/code&gt; is the string &lt;code&gt;@@redux/INIT&lt;/code&gt; (note that in the &lt;code&gt;4&lt;/code&gt;+ releases this also includes a randomly generated alphanumeric string, like &lt;code&gt;3.o.k.a.q.1.v.5.x.q.s.b.6.r.d.i.y.6.6.r&lt;/code&gt;). The Redux source notes about these action types:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;These are private action types reserved by Redux.&lt;br /&gt;
For any unknown actions, you must return the current state.&lt;br /&gt;
If the current state is undefined, you must return the initial state.&lt;br /&gt;
Do not reference these action types directly in your code.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;I don’t expect my code to be familiar with the action type &lt;code&gt;@@redux/INIT&lt;/code&gt;, so my &lt;code&gt;reducer&lt;/code&gt; should return the initial state on first call and current state on subsequent calls - this makes sense, as the code comment before the first &lt;code&gt;dispatch&lt;/code&gt; says its purpose is for every reducer to return its initial state.&lt;/p&gt;

&lt;p&gt;This first &lt;code&gt;dispatch&lt;/code&gt; call can be seen when creating a Redux store. I’ve set a breakpoint on my simple reducer within &lt;code&gt;createStore&lt;/code&gt;, and here’s what the first invocation looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/redux-series/createStore-first-dispatch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My store is ready to be returned following this dispatch call. The returned object has 5 keys (&lt;code&gt;dispatch&lt;/code&gt;, &lt;code&gt;subscribe&lt;/code&gt;, &lt;code&gt;getState&lt;/code&gt;, &lt;code&gt;replaceReducer&lt;/code&gt;, and a &lt;code&gt;Symbol(observable&lt;/code&gt;), all of which have function values. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;getState&lt;/code&gt; is a simple function that just returns &lt;code&gt;currentState&lt;/code&gt; (one of the aforementioned declared variables that is referenced by the other functions). On the &lt;code&gt;dispatch&lt;/code&gt; call in &lt;code&gt;createStore&lt;/code&gt;, &lt;code&gt;currentState&lt;/code&gt; is set to the result from invoking the reducer passed to &lt;code&gt;createStore&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/redux-series/current-state-and-reducer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on what we know about &lt;code&gt;createStore&lt;/code&gt;, writing a simple command line app that asks for a number from user input and logs it the console is simple enough:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/563190c9c17e99c6c031260514f5c215.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In the next post, I want to discuss &lt;code&gt;dispatch&lt;/code&gt; calls after the initial one and &lt;code&gt;subscribe&lt;/code&gt;. I’ll explore these Redux features by building the command line app out to allow decrementing, incrementing, multiplying and dividing numbers from user input.&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2018/04/07/redux-create-store.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2018/04/07/redux-create-store.html</guid>
      </item>
    
      <item>
        <title>Decision Trees Introduction (ML Series, Part 4)</title>
        <description>&lt;p&gt;In my continuing attempt to automate as much as of my Daily Fantasy Sports lineup creation as possible, I’ve been exploring decision trees. I realized this technique might be valuable after listening to my thought process as it relates to fantasy sports. Generally, I’ll ask myself questions like “Has this player broken 10 rebounds the last 2 games?” or “Is this player consistently getting over 25 minutes per game?”. In addition to removing bias, a decision tree should ask better questions, improve given more data and generate insights over key features in data.&lt;/p&gt;

&lt;p&gt;I want to start off with the simplest possible example I can think of for a decision tree. I’ll use this example as an opportunity to explore the &lt;code&gt;sklearn.tree&lt;/code&gt; module. In a future post, I’ll review in depth how I constructed my DFS basketball decision tree, but you can &lt;a href=&quot;https://github.com/BenBrostoff/draft-kings-learn/blob/master/recipes/classifier.py&quot;&gt;check it out here if you’re interested&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;http://benbrostoff.github.io/2017/12/06/gradient-descent-in-simple-nn-draft.html&quot;&gt;Part III&lt;/a&gt; of my ML blog series, I reviewed how a neural network could be used to classify some data where only one feature mattered. A similar example here I think will be illustrative. I often find it easier to make the example as close to real as possible, so let’s assume we have data on a group of basketball players, and we want to label them &lt;code&gt;0&lt;/code&gt; if they scored under 20 points and &lt;code&gt;1&lt;/code&gt; if they scored over 20 in a game. The three features in the data set are &lt;code&gt;minutes&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt;; let’s assume only &lt;code&gt;minutes&lt;/code&gt; matters. Below is a simple decision tree with a fake dataset:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/e0269da7acd2d3c98859fe9a98e3ed96.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;You can see in the code snippet above that &lt;code&gt;tree.DecisionTreeClassifier&lt;/code&gt; ships with a &lt;code&gt;feature_importances_&lt;/code&gt; that lists the weight of each feature. Because &lt;code&gt;minutes&lt;/code&gt; is the only feature that matters, it’s assigned a weight of &lt;code&gt;1.0&lt;/code&gt;, while the meaningless &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; features have a weight of &lt;code&gt;0.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The decision tree correctly identifies that if the player players over 30 minutes a game, then they should score over 20 points (disclaimer: this is an unrealistic and oversimplified example). The &lt;code&gt;graphviz&lt;/code&gt; package allows you to visualize this data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/decision_tree_basic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The visualization confirms what we know to be true; if a player plays over 30 minutes, they scored over 20 points. The visualization also provides a good means to go over some terminology. The &lt;em&gt;leaf nodes&lt;/em&gt; in the decision tree are the two child-most nodes that place a player in the &amp;gt; or &amp;lt; 20 point baskets; &lt;em&gt;internal nodes&lt;/em&gt; are any non leaf nodes that divide up the data, excluding the top-most node in the tree, which is the &lt;em&gt;root&lt;/em&gt; (note that this example is kind of contrived, since there is just a root node, two leaf nodes and no internal nodes).&lt;/p&gt;

&lt;p&gt;How does &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; actually work under the hood? If you were to manually construct your own decision tree, you would have to select your own root and internal nodes. Those selections would be based on what features you believe to be most valuable in a data set; if you know that minutes are what matter most for player statistics, minutes will be the feature used at your root. In a real basketball data set, a natural starting question would be “Did the player play more than 0 minutes?” &lt;a href=&quot;https://www.youtube.com/watch?v=LDRbO9a6XPU&quot;&gt;In the words of Google’s Josh Gordon&lt;/a&gt;, the best question is the one that reduces the uncertainty the most. If know the player did not play, I can claim with absolute certainty that they scored 0 points.&lt;/p&gt;

&lt;p&gt;The decision tree algorithm makes feature selections like this based on &lt;em&gt;criterion&lt;/em&gt;, which are used to compute the importance of each attribute and then arrive at the right questions to ask. While a human would anecdotally know that good NBA players get 30+ minutes a game, a decision tree would infer it statistically via criterion. I’m going to use &lt;em&gt;Gini impurity&lt;/em&gt; as my criterion of choice here, since that is what &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; uses by default. This is actually configurable via the &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&quot;&gt;&lt;code&gt;criterion&lt;/code&gt; parameter in &lt;code&gt;sklearn&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;criterion : string, optional (default=”gini”)&lt;br /&gt;
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gini impurity, as the name suggests, is not a desirable quantity, and each node seeks to minimize it. &lt;a href=&quot;https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity&quot;&gt;Via Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a mouthful, so an example and some code should help. As noted in the wiki definition, Gini impurity is a probability, so its value must be between 0 and 1. If our data set has six players who all scored over 20 points, then only one label exists in the data set, so randomly guessing that label will be correct 100% of the time. Gini impurity is 0, since we’re never wrong. However, if three players scored less than 20 and three scored more, than guessing &amp;gt; 20 would be right half the time, and wrong half the time (as would be &amp;lt; 20). Gini impurity is 0.5 since we’re wrong half the time.&lt;/p&gt;

&lt;p&gt;Josh Gordon implements &lt;code&gt;gini&lt;/code&gt; in the following way, and the two examples work using it:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/4fe00bc9e501e5671e72363ff72a2843.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;A decision tree algorithm will construct the tree such that Gini impurity is most minimized based on the questions asked. You can actually see in the visualization about that impurity is minimized at each node in the tree using exactly the examples in the previous paragraph; in the first node, randomly guessing is wrong 50% of the time; in the leaf nodes, guessing is never wrong. When the data makes perfect sense, so does the tree.&lt;/p&gt;

&lt;p&gt;Unfortunately, data is inherently messy, and configuring the tree to work based on the data will be a subject of a future post.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that the &lt;code&gt;sklearn&lt;/code&gt; source here is &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py#L75&quot;&gt;pretty difficult at least for me to parse&lt;/a&gt; (interesting note - the base class &lt;code&gt;BaseDecisionTree&lt;/code&gt; is also the parent of &lt;code&gt;DecisionTreeRegressor&lt;/code&gt;, which can be used to make floating point number predictions and may be the subject of a future blog post), so I turned to documentation and other writing for the explanations in this blog post. Here’s a summary of the most useful ones I found&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/tree.html&quot;&gt;sklearn docs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=eKD5gxPPeY0&quot;&gt;Victor Lavrenko lecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=LDRbO9a6XPU&quot;&gt;Josh Gordon, Decision tree classifier from scratch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb&quot;&gt;Josh Gordon Jupyter notebook with tree from scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;You can find all the code from this post in this &lt;a href=&quot;https://github.com/BenBrostoff/ml-series-source/blob/master/src/4_decision_tree_intro.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2018/01/27/decision-trees-intro.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2018/01/27/decision-trees-intro.html</guid>
      </item>
    
      <item>
        <title>Gradient Descent in Simple NN (ML Series, Part 3)</title>
        <description>&lt;p&gt;&lt;em&gt;Note: This post is a WIP. I am leaving it up in its current form for feedback, and will continue to update it, hopefully removing this disclaimer within the week (12/6).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This post builds on &lt;a href=&quot;https://benbrostoff.github.io/2017/10/09/gradient-descent-intuition/&quot;&gt;Part II of my ML series&lt;/a&gt; , and explores what role gradient descent plays in a simple neural network (NN).&lt;/p&gt;

&lt;p&gt;I will be working off the simple one layer NN in Andrew Trask’s blog post &lt;a href=&quot;http://iamtrask.github.io/2015/07/12/basic-python-network/&quot;&gt;A Neural Network in 11 Lines of Python (Part I)&lt;/a&gt;. First, I’m going to review part of Andrew’s posts and explain why this network makes a correct prediction give a certain simple pattern. Next, I’ll create a pattern where the network cannot predict the right answer, and attempt to explain why. In both cases, I will refer back to Part II and build on the purpose of gradient descent. I will be working with &lt;a href=&quot;https://github.com/BenBrostoff/ml-series-source/blob/master/src/3_gradient_descent_in_practice.ipynb&quot;&gt;this IPython notebook&lt;/a&gt;, and it may help to have it open in a separate window.&lt;/p&gt;

&lt;p&gt;For those unfamiliar with Andrew’s blog post, here’s a quick overview. Andrew creates a fake dataset where each data point is a Python list of three elements. If the first element is &lt;code&gt;1&lt;/code&gt;, then the data point should be labeled &lt;code&gt;1&lt;/code&gt;. If not, it’s labeled &lt;code&gt;0&lt;/code&gt;. A quick visual overview of four example inputs and outputs, which will serve as our training set. Later, we can test the model on the remaining 4 of the eight possible examples of this pattern (2 possibles in each slot -&amp;gt; &lt;code&gt;2**3&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/inputs_and_outputs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Simple and Working NN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Andrew implements a simple neural network in Python using only &lt;code&gt;numpy&lt;/code&gt; for some basic utility functions (e.g. taking the dot product of two arrays via &lt;code&gt;np.dot&lt;/code&gt;). I want to be clear on what the terminology “implements a simple neural network” means here. The only goal of this exercise is to get three weights - one for each slot - that can be used to make a prediction. Neural network in this case just refers to the methodology for getting these weights. Because the technique to obtain these weights uses parts that are named neurons and activations and layers and is loosely modeled to look like groups of neurons firing, the technique as a whole is a neural network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 1: Randomize Weights&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The weights could simply be guessed at. Random guessing does play some role in neural networks (importantly, the guessing is not exactly random and is far beyond the scope of this post), and the initial guess at the final weights leverages &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.randn.html&quot;&gt;&lt;code&gt;np.random.randn&lt;/code&gt;&lt;/a&gt;. Note that this network uses &lt;code&gt;np.random.seed(1)&lt;/code&gt; so outputs are consistent. The first guess at the weights computes &lt;code&gt;2 * np.random.random((3, 1)) - 1&lt;/code&gt; - the 2 is simply to scale the weights by 2, which are all between 0 and 1. The &lt;code&gt;-1&lt;/code&gt; then forces any weights that were initialized as less than &lt;code&gt;0.5&lt;/code&gt; to be negative (e.g. &lt;code&gt;2 * 0.4 - 1 == -0.2&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The first guess at the weights suggests they’re &lt;code&gt;-0.17&lt;/code&gt; (too small), &lt;code&gt;0.44&lt;/code&gt; (too big) and &lt;code&gt;-0.99&lt;/code&gt; (too small). Because we know that the first slot is the only one that matters, the correct weights are &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt;, or some large number and two small numbers. A neural network &lt;em&gt;should&lt;/em&gt; move the first weight upward, the second weight downward and the third weight upward.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 2: Guess&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;What is the output when we guess with the weights? The answer to this question is computed via:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;predictions = np.dot(training_set_inputs, weights)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The output here is a &lt;code&gt;numpy&lt;/code&gt; array of shape &lt;code&gt;(4, 1)&lt;/code&gt; (&lt;code&gt;training_set_inputs&lt;/code&gt; - four examples of three elements - is &lt;code&gt;(4, 3)&lt;/code&gt; and the &lt;code&gt;weights&lt;/code&gt; - three “predictors” - are &lt;code&gt;(3, 1)&lt;/code&gt;). It represents guesses at the four labels given the four examples in the training set. Of course, these guesses - &lt;code&gt;-2.5&lt;/code&gt;, &lt;code&gt;2.74&lt;/code&gt;, &lt;code&gt;2.95&lt;/code&gt;, and &lt;code&gt;-2.73&lt;/code&gt; - are quite far away from the actual labels of &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I want to take a quick second to do some discussion of &lt;code&gt;np.dot&lt;/code&gt;. For those with a linear algebra background, feel free to skip this section; I’m an Economics major so &lt;code&gt;np.dot&lt;/code&gt; has taken me some time to wrap my head around. The &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html&quot;&gt;documentation&lt;/a&gt; around &lt;code&gt;np.dot&lt;/code&gt; makes it sound deceivingly simply:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dot product of two arrays. For 2-D arrays it is equivalent to matrix multiplication, and for 1-D arrays to inner product of vectors (without complex conjugation). For N dimensions it is a sum product over the last axis of a and the second-to-last of b:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The matrix math explanation makes the most sense here, and the formula you would apply for matrix math does indeed work:&lt;/p&gt;

&lt;p&gt;That said, I think the intuition here is important - why are we using &lt;code&gt;np.dot&lt;/code&gt;? We know &lt;em&gt;where we want to end up&lt;/em&gt; is four predictions, or a &lt;code&gt;numpy&lt;/code&gt; array of shape &lt;code&gt;(4, 1)&lt;/code&gt;. To get each prediction, we need to apply the weights. Getting the sum weighted prediction would look like the below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First prediction: &lt;code&gt;0 * -0.17 + 0 * 0.44 + 1 * -0.99 == -0.99&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Second prediction: &lt;code&gt;1  * -0.17 + 1 * 0.44 + 1 * -0.99 == -0.73&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Third prediction: &lt;code&gt;1 * -0.17 + 0 * 0.44 + 1 * -0.99 == -1.17&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Fourth prediction:  &lt;code&gt;0 * -0.17 + 1 * 0.44 + 1 * -0.99 == -0.56&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is all &lt;code&gt;np.dot&lt;/code&gt; is doing here. &lt;code&gt;predictions&lt;/code&gt; is &lt;code&gt;[[[-0.99977125], [-0.72507825], [-1.16572724], [-0.55912226]]]&lt;/code&gt;. The linear algebra intuition for &lt;code&gt;np.dot&lt;/code&gt; is a little different, and I have found &lt;a href=&quot;https://www.youtube.com/watch?v=kjBOesZCoqc&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;3Blue1Brown’s linear algebra series&lt;/a&gt; worth watching here (disclaimer: I’m only four videos through). But at least in my minimal machine learning experience, I have found it more useful to stop thinking about rote learning matrix math formulas and referring back to the predictions intuition.&lt;/p&gt;

&lt;p&gt;A little terminology here - &lt;code&gt;predictions&lt;/code&gt; is really the first hidden layer of the neural network, and also the output layer here as the whole network is only a single input and out layer. In &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Andrew Ng’s deeplearning course&lt;/a&gt;, the step is represented as:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Z1 = W1 * X&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where Z1 is the output layer, W1 are weights and X is the training inputs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 3: Apply a Non Linearity and Find Error&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These four numbers from &lt;code&gt;predictions&lt;/code&gt; can now be passed through a non-linearity, or function that does not have the same effect on the output as the input is incremented or decremented. &lt;em&gt;Why&lt;/em&gt; passing &lt;code&gt;predictions&lt;/code&gt; through a non-linearity is important will likely be the subject of another post. For now, I think it’s sufficient to just say that we will pass the four predictions through a function called a sigmoid function that scales them from 0 to 1 and &lt;a href=&quot;http://www.wolframalpha.com/input/?i=sigmoid+function&quot;&gt;looks like the below&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/sigmoid.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, in Andrew Ng’s deeplearning course, this looks like:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;A1 = G(Z1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where G is the non-linearity (sigmoid function in this example). &lt;code&gt;A1&lt;/code&gt; is called a layer of activations, because the non-linearity is thought to “activate neurons” in the loose brain metaphor.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 4: Apply Gradient Descent&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 5: Update the Weights&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Step 6: Repeat&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Simple and Broken NN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This network breaks when changing the rule for inputs and outputs.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/12/06/gradient-descent-in-simple-nn-draft.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/12/06/gradient-descent-in-simple-nn-draft.html</guid>
      </item>
    
      <item>
        <title>Commits - Not a Proxy for Progress</title>
        <description>&lt;p&gt;I’m resolving to stop using commits as a measure of progress.&lt;/p&gt;

&lt;p&gt;Generating code is easy. Generating code while understanding every line is hard. This may on the surface seem like an absurd statement, especially if new code has no library dependencies. But native code for a specific language has its own complexities. A recent example I ran into - &lt;a href=&quot;https://stackoverflow.com/questions/21771220/error-handling-with-node-js-streams&quot;&gt;error handling with Node &lt;code&gt;stream&lt;/code&gt;s&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Good writers talk all the time about all the work that comes before and after writing. &lt;a href=&quot;https://tim.blog/2016/05/22/sebastian-junger/&quot;&gt;Sebastian Junger&lt;/a&gt; is insistent on understanding his subject matter as much as possible before putting pen to paper. &lt;a href=&quot;https://www.amazon.com/Writing-10th-Anniversary-Memoir-Craft/dp/1439156816&quot;&gt;Stephen King&lt;/a&gt; urges writers to tear apart their initial and even later drafts as the editing process creates insight. Coding should be no different.&lt;/p&gt;

&lt;p&gt;The before and after work for coding should aim to reduce code generated to as small a volume as possible. This process takes time - time to consider different architectures, &lt;a href=&quot;http://benbrostoff.github.io/2017/08/05/rtd/&quot;&gt;time to read documentation&lt;/a&gt; and time to receive and address feedback. In no way does this time correlate with commits, nor would correlation be desirable. Version control should tell the story of what changed in the code, not the story of someone’s coding process. This is the difference between &lt;code&gt;Adds error handling to batch script&lt;/code&gt; and three commits that are different iterations of error handling.&lt;/p&gt;

&lt;p&gt;What can substitute for commits then as a measure of progress on a project? I see value in tools that track the before and after process, in addition to the actual writing of code. I have found &lt;a href=&quot;https://qotoqot.com/qbserve/&quot;&gt;&lt;code&gt;Qbserve&lt;/code&gt; useful here&lt;/a&gt; - it’s a tool that shows time spent in different applications. Good old-fashioned “software journaling” is another technique I love - I especially enjoyed this &lt;a href=&quot;http://winterflower.github.io/2017/08/17/software-engineering-notebook/&quot;&gt;blog post on it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What gets measured gets managed. Let’s make sure we’re measuring the right things.&lt;/p&gt;
</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/11/09/commits-are-not-a-proxy-for-progress.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/11/09/commits-are-not-a-proxy-for-progress.html</guid>
      </item>
    
      <item>
        <title>Gradient Descent Intuition (ML Series, Part 2)</title>
        <description>&lt;p&gt;How do machines learn?&lt;/p&gt;

&lt;p&gt;One answer is by getting fewer answers wrong over time. One technique for decreasing wrongness is &lt;em&gt;gradient descent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In this post, I want to explore gradient descent as a standalone topic. In the next post in this series, I’ll discuss its relationship with machine learning, and run through a basic example of why gradient descent is useful in helping machines get fewer answers wrong.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gradient Descent - Find The Minimum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matt Nedrich over at Atomic Object has &lt;a href=&quot;https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/&quot;&gt;an excellent definition of gradient descent&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;At a theoretical level, gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved using calculus, taking steps in the negative direction of the function gradient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gradient descent can be thought of as &lt;a href=&quot;http://mathworld.wolfram.com/Gradient.html&quot;&gt;a synonym for slope&lt;/a&gt;. Since slope is rise / run, and a horizontal line has a slope of 0 (0 rise over infinite run), finding the slope at the function’s minimum can be thought of as “descending” to a gradient of zero. The name gradient descent makes sense, because once the gradient decent algorithm has run for a given function, the computed parameters will descend the slope at the computed point to zero.&lt;/p&gt;

&lt;p&gt;Providing clear examples of the &lt;em&gt;how&lt;/em&gt; of ML are among the reasons I’m writing this &lt;a href=&quot;http://benbrostoff.github.io/2017/09/19/why-ml/&quot;&gt;blog series&lt;/a&gt;, so let’s use gradient descent on the following function:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(x - 5) ** 2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I write this function in this form as opposed to &lt;code&gt;x ** 2 - 10x + 25&lt;/code&gt; because it’s easy to see that the function is minimized at 5. Gradient descent should return &lt;code&gt;5&lt;/code&gt; for &lt;code&gt;x&lt;/code&gt; if the algorithm lives up to its name.&lt;/p&gt;

&lt;p&gt;The gradient descent algorithm itself is all of one line, but importantly requires 1) the derivative (also synonymous with slope) of the function and 2) a small number to descend the gradient. The small number - which we’ll set to &lt;code&gt;0.01&lt;/code&gt; in this example - is necessary because it allows the gradient to move on each iteration of the algorithm. The derivative can be &lt;a href=&quot;https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules/ab-diff-negative-fraction-powers/a/power-rule-review&quot;&gt;computed through the power rule&lt;/a&gt;, which I had to review via the provided link from Khan academy. In short, moving the exponent &lt;code&gt;2&lt;/code&gt; down and raising the result to &lt;code&gt;2-1&lt;/code&gt; returns a derivative function of &lt;code&gt;2(x-5) ** 1&lt;/code&gt; or &lt;code&gt;2x - 10&lt;/code&gt;. We now have the parameters necessary to run gradient descent for a few iterations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/first_iterations.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What this algorithm does is 1) take the previous minimized value (or start at zero) and 2) subtract the slope function with the previous minimum value passed in, multiplied by the small number.&lt;/p&gt;

&lt;p&gt;And if run for 10,000 iterations or even 100,000 iterations, with &lt;code&gt;0.01&lt;/code&gt; as the small number, &lt;code&gt;x&lt;/code&gt; converges to &lt;code&gt;5&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/converge.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what if instead of a small number we used a huge number? What is clear from experimentation is that numbers larger than one will never allow the solution to converge to &lt;code&gt;5&lt;/code&gt;. While a small number of &lt;code&gt;0.99&lt;/code&gt; converges to &lt;code&gt;5&lt;/code&gt;, a small number of &lt;code&gt;1.0&lt;/code&gt; bounces the solution back and forth between &lt;code&gt;10&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; forever. A couple of iterations in this sequence are instructive:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/ml-blog-series/bad_learning_rate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The value selected for the small number is extremely important. Selection of the small number is an example of &lt;em&gt;hyperparamater tuning&lt;/em&gt; in machine learning, which we’ll explore in a later post.&lt;/p&gt;

&lt;p&gt;Please note all the code for this post can be found &lt;a href=&quot;https://github.com/BenBrostoff/ml-series-source&quot;&gt;on my GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Does Gradient Descent Help Machines Learn?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The small number mentioned above is called the &lt;em&gt;learning rate&lt;/em&gt; in ML contexts, and drives the learning in machine learning. The function the gradient descent algorithm is applied on is the &lt;em&gt;loss function&lt;/em&gt;, where loss is the difference between the prediction made from a machine learning example and actual reality. For example, if an ML model predicted an NFL running back will finish with &lt;code&gt;17&lt;/code&gt; touch downs for the 2016 season and the reality is &lt;code&gt;11&lt;/code&gt;, the loss in this example is &lt;code&gt;6&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the next post, I’m excited to explore an actual application of gradient descent in machine learning.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/10/09/gradient-descent-intuition.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/10/09/gradient-descent-intuition.html</guid>
      </item>
    
      <item>
        <title>Why I'm Writing About ML (ML Series, Part 1)</title>
        <description>&lt;blockquote&gt;

  &lt;p&gt;At the heart of science is an essential balance between two seemingly contradictory attitudes - an openness to new ideas, no matter how bizarre or counterintuitive, and the most ruthlessly skeptical scrutiny of all ideas, old and new.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Carl Sagan, &lt;a href=&quot;https://www.amazon.com/Demon-Haunted-World-Science-Candle-Dark/dp/0345409469&quot;&gt;The Demon-Haunted World&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;I know of no better tool than writing to explore my own understanding of technical subjects. After years of reading about machine learning in the press, programmer blogs and from technology leaders of our time, I want to investigate ML on my own terms. This investigation may take months or years or, based on how quickly the field is changing, forever - I’m not sure. What I can say is that my goal in writing a multi-part series is no less than what Sagan describes in the above quote. I intend to be both open to extraordinary claims about machine learning and simultaneously skeptical.&lt;/p&gt;

&lt;p&gt;The value I believe I can add in writing this series is in infusing each post with as much illustrative code as possible. I want to explore not only programming libraries, but the cloud services that allow library code to be run with GPU enabled computing power. While I have been extraordinarily influenced by previous explorations of ML like Tim Urban’s &lt;a href=&quot;https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html&quot;&gt;AI Revolution Series&lt;/a&gt;, these posts are going to be technical and experiment heavy.&lt;/p&gt;

&lt;p&gt;I want to answer for myself not only &lt;em&gt;why&lt;/em&gt; machine learning works, but also the various &lt;em&gt;hows&lt;/em&gt;. I will try to approach as much as I can from first principles in answering &lt;em&gt;why&lt;/em&gt; machine learning is effective. Where ML relies on ideas from linear algebra and calculus, I promise to explore these fields as well. On the subject of &lt;em&gt;how&lt;/em&gt; machine learning works today, I want to start first with as few libraries as possible, and then move slowly into popular libraries like &lt;code&gt;TensorFlow&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What these posts will &lt;em&gt;not&lt;/em&gt; do is engage in futurism discussions or projections of what machine learning could do. I am going to stay focused on what machine learning &lt;em&gt;can do at present&lt;/em&gt;. Any demonstrations of machine learning capabilities I’ll post on GitHub. As someone with no formal ML background, I encourage anyone (un)lucky enough to be reading these posts to raise issues and provide guidance on what I could be doing better.&lt;/p&gt;

&lt;p&gt;Along the way, I’ll be diving into the following resources. I’ll add to this list as this series takes shape:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Andrew Ng’s deep neural network &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Coursera specialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sebastian Raschka’s &lt;a href=&quot;https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0&quot;&gt;Python Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, I want to manage expectations about how often I’ll be posting in this series and post lengths. &lt;a href=&quot;https://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692&quot;&gt;Deep Work&lt;/a&gt; is one of my favorite books, and in the spirit of Cal Newport’s suggestion to budget deep work hours to projects, I intend to devote 3-5 hours of deep work per week to this project. In terms of length, these posts might be short or long depending on the idea being explored. I have set a short-term goal of posting once per week for the next 5 weeks; after that, I’ll evaluate the quality of those posts and make improvements for the next ones.&lt;/p&gt;

&lt;p&gt;I’m excited to get started. If these posts can change my own opinion on machine learning, than they were worth doing. If anyone else reads them, it’s an added bonus.&lt;/p&gt;

&lt;p&gt;Here’s to openness and ruthless skepticism.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/09/19/why-ml.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/09/19/why-ml.html</guid>
      </item>
    
      <item>
        <title>RTD</title>
        <description>&lt;p&gt;RTD - Read the Docs. That phrase cuts across industries - I have heard it just as much as a programmer as I did as an investment banker. And while when said verbally, RTD (sometimes RTFD) usually lands with a hint of annoyance, I really believe it’s some of the most important career advice I have ever received.&lt;/p&gt;

&lt;p&gt;I recently was writing some unit tests to check that a small wrapper around the &lt;a href=&quot;https://aws.amazon.com/documentation/ses/&quot;&gt;AWS.SES API&lt;/a&gt; was invoked correctly and logging error messages as expected. Using &lt;a href=&quot;https://github.com/sinonjs/sinon&quot;&gt;&lt;code&gt;sinon&lt;/code&gt;&lt;/a&gt; and the &lt;a href=&quot;https://github.com/dwyl/aws-sdk-mock&quot;&gt;&lt;code&gt;aws-sdk-mock&lt;/code&gt;&lt;/a&gt; library, I had some code that looked like the below to check error handling:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/1cba16104882f0c73609ff105ee39d5d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;As background, &lt;code&gt;mock&lt;/code&gt; from &lt;code&gt;aws-sdk-mock&lt;/code&gt; takes three arguments, the third of which is a function that, in the case of mocking &lt;a href=&quot;http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SES.html#sendEmail-property&quot;&gt;sendEmail&lt;/a&gt;, takes the email params and a callback. The callback itself takes two arguments - an error message and data from the response.&lt;/p&gt;

&lt;p&gt;I received a comment from my boss on this code review to the effect that every time I was using &lt;code&gt;callsFake&lt;/code&gt;, I could just be using &lt;a href=&quot;http://sinonjs.org/releases/v3.0.0/stubs/&quot;&gt;&lt;code&gt;callsArgWith&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;http://sinonjs.org/releases/v3.0.0/stubs/&quot;&gt;documentation&lt;/a&gt; for &lt;code&gt;callsArgWith&lt;/code&gt; points to &lt;code&gt;callsArg&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;stub.callsArgWith(index, arg1, arg2, …)&lt;/p&gt;

  &lt;p&gt;Like callsArg, but with arguments to pass to the callback.&lt;/p&gt;

  &lt;p&gt;stub.callsArg(index)&lt;/p&gt;

  &lt;p&gt;Causes the stub to call the argument at the provided index as a callback function. stub.callsArg(0); causes the stub to call the first argument as a callback.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;It made sense - my only goal was to fake invoking the callback &lt;code&gt;sendEmail&lt;/code&gt; takes following resolution of the email &lt;code&gt;Promise&lt;/code&gt;. Giving it an error didn’t require passing in a new function to &lt;code&gt;callsFake&lt;/code&gt; - &lt;code&gt;sinon&lt;/code&gt; already offers this functionality:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sendEmailStub = sandbox.stub().callsArgWith(1, new Error());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And because you’re always one click a way in your IDE from &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/lib/sinon/default-behaviors.js#L55&quot;&gt;seeing the source&lt;/a&gt; in &lt;code&gt;node_modules&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/14244abba5cb9ee47dac6884bc862abf.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;after which there’s an easy path to cloning the source repo and after some investigation finding your way to &lt;code&gt;callCallback&lt;/code&gt; in &lt;code&gt;lib/sinon/behavior.js&lt;/code&gt;, which gets the callback using &lt;code&gt;callArgAt&lt;/code&gt; set in the source we looked at, which then takes the &lt;code&gt;callbackArguments&lt;/code&gt; sans the &lt;code&gt;fake&lt;/code&gt; and &lt;code&gt;position&lt;/code&gt; arguments passed in &lt;code&gt;callsArgWith&lt;/code&gt; (note the function signature in the docs differs from what is the source - this is because &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/lib/sinon/behavior.js#L204&quot;&gt;&lt;code&gt;addBehavior&lt;/code&gt;&lt;/a&gt; will always add functions to the stub prototype with the first argument being the stub itself).  &lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://github.com/sinonjs/sinon/blob/master/test/stub-test.js#L574&quot;&gt;here’s the test in &lt;code&gt;sinon&lt;/code&gt;&lt;/a&gt;, clearly demonstrating that a callback passed as the second argument to a stub should be able to be passed any argument with &lt;code&gt;callsArgWith&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/5951ea3710240f413739438620148022.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In looking at the &lt;code&gt;sinon&lt;/code&gt; docs and then the Amazon SES docs, I realized there was probably an ocean of API functionality I was not taking advantage of. And the only way to find out would be to study and experiment. Which is the fun of being an engineer.&lt;/p&gt;

&lt;p&gt;I bring up this example because I think it illustrates the importance of reading documentation (and even better, source code). I was familiar with &lt;code&gt;callsFake&lt;/code&gt;, so every testing situation looked like a nail to its hammer. This is what happens when API docs are read quickly and with the intent of completing a task. The cost is that robust APIs are used incorrectly, and code that should be short and sweet becomes long and ugly.&lt;/p&gt;

&lt;p&gt;I know this seems obvious, but I wanted to write about it because I know there are different gradients of RTD. In banking, you might read the earnings call transcript but not the 10-Q; only the part of the S-1 with the risks to the business; the first tab of the huge Excel model.&lt;/p&gt;

&lt;p&gt;In programming, there’s cherry-picking documentation; reading the documentation without the source code; starting a project with “getting started” documentation and never revisiting the docs again, and so much more.&lt;/p&gt;

&lt;p&gt;A lot of this can be argued against with allusions to time management principles. How am I supposed to get anything done if I’m reading forty thousands pages of documentation and source code? Obviously, reading all the documentation - especially for something like the AWS SDK - is impossible. But transferring a 3 hour chunk of time from 70 / 30 writing code / reading docs to 65 / 35 writing code / reading docs?&lt;/p&gt;

&lt;p&gt;You might end up improving your code and reading some interesting source code from a great library  &lt;/p&gt;
</description>
        <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/08/05/rtd.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/08/05/rtd.html</guid>
      </item>
    
      <item>
        <title>Upgrades</title>
        <description>&lt;p&gt;I bought a new MacBook Pro 2016 today, which represents a large upgrade for me. I was on an MBP 2012 before. The performance improvements are remarkable. Just using it today felt like one of those (ever more frequent) times when technology shocks you with how quickly it improves. I got to thinking about how many open source tools just in the last few years have sped up my workflow as a developer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;Anaconda&lt;/li&gt;
  &lt;li&gt;Jupyter notebooks&lt;/li&gt;
  &lt;li&gt;ES6 (especially &lt;code&gt;async / await&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;yarn&lt;/li&gt;
  &lt;li&gt;React and React Native&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the last decade (almost half decade), at some point one of these things did not exist. I could not envision myself living without any one of them now.&lt;/p&gt;

&lt;p&gt;As someone who uses a lot of open source tools, I feel very privileged. I am the beneficiary of frequent upgrades. When I need to debug someone else’s library, it’s generally not too terrible, and almost half the time there’s a robust conversation on GitHub about the exact issue I’m having. The other half of the time, I’m probably using the API incorrectly. Being a developer is fun because nothing stays broken for that long.&lt;/p&gt;

&lt;p&gt;But yes, more things do break, and complexity increases. That’s a given. And Hacker News, reddit and Twitter will remind you ad nauseam that everything is terrible and you’re part of the problem.&lt;/p&gt;

&lt;p&gt;I sometimes worry that these channels have become so vocal that they’re discouraging would-be makers from contributing. &lt;a href=&quot;https://www.kennethreitz.org/essays/the-reality-of-developer-burnout&quot;&gt;Kenneth Reitz wrote recently about publish-only mode&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;I didn’t want to lose what I valued most about my position within our community — being able to influence the world I cared so much about. So, I unfollowed everyone on Twitter. Every single person. I stopped paying attention to tech trends and reading hacker news. I went into publish-only mode.&lt;/p&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;I continue to find these words inspirational, because I think they’re more relevant than ever. The open source ecosystem is meant to be lived in and improved. To the extent that developers can adopt improved software with as little friction as possible, I believe the world becomes a better place.&lt;/p&gt;

&lt;p&gt;I was reminded today that software and hardware improvements can really impact human life in a positive way. I want to help bring upgrades into the open source ecosystem as much as I can. And the road to upgrades lies in &lt;a href=&quot;https://www.amazon.com/dp/B00X47ZVXM/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1&quot;&gt;deep work&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, here’s to publish-only mode, and resolving to be a part of the next decade of upgrades.  &lt;/p&gt;
</description>
        <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
        <link>http://benbrostoff.github.io//2017/05/06/upgrades.html</link>
        <guid isPermaLink="true">http://benbrostoff.github.io//2017/05/06/upgrades.html</guid>
      </item>
    
  </channel>
</rss>