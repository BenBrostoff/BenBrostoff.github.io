<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ben Brostoff</title>
    <description></description>
    <link>http://0.0.0.0:4000</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Strategies for Long Projects</title>
        <description>&lt;p&gt;I’m in the middle of three multi-month to a year projects right now:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Writing a book&lt;/li&gt;
  &lt;li&gt;Running a marathon&lt;/li&gt;
  &lt;li&gt;Building a new API&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each I started months ago and have found brutally difficult in unique ways. For each, progress is often times non-visible and the visible progress is incremental. Running is frequently an exercise in watching your pace go up and down with no discernible reason for the zigs and zags. Many days feel like regressions. Direction can and will change - in writing my book, I’ve changed the plot significantly three different times. Each resulted in a substantial rewrite. Project goals are often moving targets. As is the case with almost everything, unexpected obstacles crop up that you didn’t forecast. Software is a living example here - when you have dependencies (and what software project doesn’t?), changes in those dependencies are changes you must adjust to.&lt;/p&gt;

&lt;p&gt;The saving grace here is that simply putting in the hours usually moves you closer to the goal. Failure is higher probability when time or effort on a project decreases. As a result, I think time budgeting and attitude make an enormous difference. Most of the ideas below deal with one or both. As I like to point out in all of my blog posts that prescribe behavior, the below is stuff that has worked for me and your mileage may vary. Here we go:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relentless, irrational optimism is the only attitude that works&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Something that seems trivial turns out to be nearly impossible. Mile 5 of a 20 mile run hurts like heck. A third party library massively changed its API. That programming language feature you were confident would resolve your issue doesn’t behave the way the docs said. Unexpected events are going to happen, and while you can’t control these events, you can control your response to them.&lt;/p&gt;

&lt;p&gt;“In the space between stimulus (whap happens) and response, lies our freedom to choose,” &lt;a href=&quot;https://www.goodreads.com/quotes/459654-in-the-space-between-stimulus-what-happens-and-how-we&quot;&gt;writes Stephen Covey&lt;/a&gt;. Taking the time to craft your response when an unexpected event occurs is the opportunity to frame that event as opposed to letting that event frame you. I can’t stress how important I think this idea is - a bad reaction to new information can kill a project in its tracks. I look back on a few of the side projects I’ve stopped working on because I found some library or business that was doing the same thing. In retrospect, I understand that markets can have a large number of competing participants (check out the 10 different rent-a-cars near any major airport). It’s human nature to react negatively to information that isn’t congruent with our expectations. Taking time to respond to new information in addition to expecting the unexpected are strong defenses against negativity.&lt;/p&gt;

&lt;p&gt;Moreover, I believe that choosing to feel something can make you feel that way even if the feeling is artificially manufactured. What I mean by this is that when someone asks us to label how we feel, the label we select is based on how we physically feel at the moment. But what if you said the exact opposite of how you actually felt? Is it possible the re-labeling could become reality? This seems absurd on the face of it, but my experience has been that re-labeling works and causes an actual physical response.&lt;/p&gt;

&lt;p&gt;Of course, taking time and choosing a response is only part of the equation. The other part is infusing such responses with optimism, even if the optimism is a little irrational. I’m not advocating here for pie in the sky optimism - I’ll never run a sub four minute mile. What I am advocating for is optimism that represents a reasonable and possible outcome, even if a few things would have to fall into place for that outcome to happen. After all, our negative responses generally &lt;em&gt;assume&lt;/em&gt; a few things would have to happen for that outcome to play out, so it’s fair to make assumptions on the upside.&lt;/p&gt;

&lt;p&gt;As an example, let’s assume some third party you rely on changed its API and is deprecating the old API in a month. A negative response would implicitly assume that the required changes would take X days and just cause churn in the codebase. A positive response might assume Y days (&amp;lt; X) and that the new API included performance and other improvements AND that doing this refactor could be the subject of a blog post or tech talk that could be used to recruit great engineers or land consulting contracts.&lt;/p&gt;

&lt;p&gt;For the relentless optimist, every setback is 1) never as bad as it seems on the surface and 2) an opportunity in disguise. This view is healthier than the alternative, which is guaranteed to sap energy and fail to identify opportunities embedded in the unexpected event.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documenting each day shows progress that would otherwise be hidden&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CHANGELOG&lt;/code&gt;s in software are great. They serve as a living document that traces the life of a project. It answers the question “What have the maintainers of this project been doing?” A personal changelog or journal can answer these same questions and prevent the negative feelings that can arise from feeling that a project is going nowhere.&lt;/p&gt;

&lt;p&gt;The reason these feelings arise can be that we just forget the past. For software, I’ve found this is often the time it takes to learn a new technology or understand the source code from a project you’re expected to contribute to. Both take significant time, but once you’ve acquired the skills, it then becomes easy to say “Why did &lt;taskX&gt; take me so long?&quot; Personal changelogs I think should be extremely detailed and include things like:&lt;/taskX&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Spent five hours going through source of repo Y and looking at Stack Overflow and documentation. Tried and failed several times to refactor code - got some cryptic errors with a stack trace I didn’t understand. Finally talked to Bob and found out some this is expected behavior and the workaround is to use a forked version of library XYZ AND to set some environment variables to specific values. This isn’t documented anywhere, so I took the time to document it. Still working on refactoring code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The fact of the matter is that starting a project is extremely difficult - perhaps more difficult than the middle and end stages of long projects. Learning new things and creating new ideas and patterns requires time, often far more time than implementing the ideas and pattern.&lt;/p&gt;

&lt;p&gt;When a project is a group project, this time might even have some multiplier on it. &lt;em&gt;Agreeing&lt;/em&gt; on new patterns and ideas is perhaps no less difficult than thinking of those ideas and patterns in the first place. A mentor of mine also showed me recently that new groups often regress a bit in the early stages of forming new practices and adjusting to each others’ work patterns - &lt;a href=&quot;https://en.wikipedia.org/wiki/Tuckman%27s_stages_of_group_development&quot;&gt;these stages are called “forming” and “storming”&lt;/a&gt;. Group dynamics take time to get right.&lt;/p&gt;

&lt;p&gt;Changelogs are a way to document things that are easy to forget. When I get discouraged at the pace of a project, looking at my changelog / journal often helps me understand why things have played out the way they have.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compounding matters a lot&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Compounding generally is discussed in a financial context where interest accrues interest. For software, the equivalent here is developer tools you took time to write being used by you and your teams (code gen, scripts, deploy orchestration, etc.). For running, it’s increasing your V02max and being able to sustain the same effort for a longer time. In each case, the initial time investment saves time down the line and results in new opportunities that may save even more time. Time saved results in new work that saves more time that results in more new work and so on. This is a positive feedback loop.&lt;/p&gt;

&lt;p&gt;David Goggins in his book &lt;a href=&quot;https://www.amazon.com/Cant-Hurt-Me-Master-Your/dp/1544512287&quot;&gt;&lt;em&gt;Can’t Hurt Me&lt;/em&gt;&lt;/a&gt; talks about a memory bank that can be drawn on in times of adversity (or really any time). For each obstacle conquered in pursuit of your goal, you have created a memory that can help you overcome future obstacles. I’ve found this most evident in running - continually doing long runs has made it easier to push through pain. “I can’t believe I have four miles to go” over time has slowly become “I only have four miles to go”. The memory bank re-frames what mileage means. Just like compound interest, positive memories help accrue more positive memories which accrue more positive memories. Some athletes call this “building a base”. The base sets what is normal. By continually increasing the floor that is normal behavior, long-term projects can deliver compounding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finding the time means being extremely defensive of your time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Time is finite. It can be uncomfortable to accept this and act as a fierce guardian of your own time, as doing so may include declining social invitations. I recommend watching &lt;a href=&quot;https://www.youtube.com/watch?v=arj7oStGLkU&quot;&gt;Tim Urban’s TED talk&lt;/a&gt; on procrastination. Tim envisions time as a series of 10 minutes blocks - giving up any of those blocks mean that block is lost forever.&lt;/p&gt;

&lt;p&gt;I think when I’ve talked about this point with friends and family, their reaction is that doing this means becoming an anti-social hermit and making some tradeoff that’s harmful to you. That hasn’t been my experience when I’ve acted in defense of my long-term projects. In fact, &lt;em&gt;not&lt;/em&gt; acting in defense of my long-term projects has felt harmful to me. Falling behind on a project that’s important to me introduces feelings of guilt and frustration.&lt;/p&gt;

&lt;p&gt;Importantly, I think life’s many priorities can be balanced with long-term projects. It has been written that Walter Isaacson wrote many of his books on vacations with friends and family. He would slip away for a few hours at a time, and no one really noticed. He had a great time on these vacations and it seems like nobody was negatively impacted by his small absences.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I wanted to conclude by saying I truly believe long-term projects can lead to transformative returns, even if they fail. Trying to run a marathon, for instance, has brought me into contact with dozens of runners at work and in a running club I go to I probably never would’ve met otherwise. Meeting these runners has led to relationships that have helped me on non-running projects. It’s obviously helped me a ton with my running. I think I’ve witnessed first hand that ambitious projects generate supportive responses from people - on the marathon, the book and the new API, countless people have offered their support, and I’ve been glad to take it.&lt;/p&gt;

&lt;p&gt;I want to encourage myself and anyone reading this to keep trying things that are hard and take a long time. It will be tempting to quit, but overcoming that temptation could be the best decision you ever make.&lt;/p&gt;
</description>
        <pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/09/28/long-projects.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/09/28/long-projects.html</guid>
      </item>
    
      <item>
        <title>The Group Effect</title>
        <description>&lt;p&gt;I first read about the Group Effect in Matt Fitzgerald’s book &lt;a href=&quot;https://www.amazon.com/How-Bad-You-Want-Psychology/dp/1937715418&quot;&gt;&lt;em&gt;How Badly Do You Want It&lt;/em&gt;&lt;/a&gt;, a collection of stories about endurance athletes who overcame incredible odds. &lt;a href=&quot;https://mensrunninguk.co.uk/top-feature/the-group-effect/&quot;&gt;Fitzgerald summarizes&lt;/a&gt; the group effect below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The tiny of island of Cuba has captured 67 Olympics medals in the sport of boxing since 1968, more than any other country. Germany has finished either first, second or third in 13 of the last 16 World Cups. Nations that dominate a particular sport share one basic characteristic: their people are crazy for that sport. To put it in a formula: national dominance of a sport is a function of the scope and intensity of its citizens’ participation in it. If enormous numbers of a nation’s people participate in a sport, that nation is certain to do quite well in global competition. Sociologist John Bruhn dubbed this phenomenon the group effect. The psychobiological model of endurance performance explains how it works in the context of sports like running.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;According to this model, any factor that reduces the amount of effort an athlete perceives at a given level of exercise intensity will enhance performance. When people work together, their brains release greater amounts of mood-lifting, discomfort- suppressing endorphins than they do when the same task is undertaken alone. Consequently, endurance athletes perceive less effort and perform better when training and racing cooperatively than they do alone. The group effect is not something that has to be acquired. It is a coping skill that exists latently in everyone, ready to be activated by the right situation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I am a believer that the group effect is real. In sports and work settings, I believe my perception of effort has been positively impacted by being in groups. Difficult tasks feel less difficult for me when there is a social element to these tasks. The shared experience of running or pair programming, for instance, makes fatigue and complexity easier to deal with because you have one or more people to share it with.&lt;/p&gt;

&lt;p&gt;Realizing the benefits of the group effect requires a group to exist. For various reasons, there may not be a group for the thing you want to improve at. And even if there is, it may not practice the thing together. An example of this group-but-not-group-effect situation would be an engineering culture where the pull request culture is to sight-read and press approve or make some small comments. Software engineer group-effect from my experience only comes from two or more people running code together and experimenting with the effects of small and large changes in an in-person setting. Said another way, just because a group does the same thing doesn’t mean they do it together.&lt;/p&gt;

&lt;p&gt;I think it’s possible that when people talk about culture, they’re actually referring to a result of the group effect. My experience has been that a culture is positive for me when 1) everyone feels comfortable, 2) everyone feels they are improving and 3) everyone feels they are adding value. The group effect directly addresses (2) - per the article above, people “perceive less effort and perform better between when training… cooperatively than when they do alone.”&lt;/p&gt;

&lt;p&gt;In a way, (3) is an extension of (2), because if everyone is improving, everyone (potentially) feels that just by virtue of showing up and putting in effort, they’re adding to each team member’s success. With regard to (1), I’m not sure what group dynamics need to arise in order for everyone to feel comfortable. I do think that spending more time together generally improves relationships between people, so more group effect may lead to more comfort in being a part of the group.&lt;/p&gt;

&lt;p&gt;Knowing the group effect exists is an argument for introducing it when possible. Yes, some activities are probably easier when performed solo. Pair-programming often feels less efficient to me than solo-programming, but I also don’t think efficiency is a proxy for consistency or resistance against quitting. Some of my biggest breakthroughs in projects have come from good pair programming sessions. There’s a place for both, and neither should be ignored or said to be always better than the other.&lt;/p&gt;

&lt;p&gt;Some potential ways in software and sports to introduce the group effect:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For code reviews - moving code reviews to in-person, interactive Q&amp;amp;A sessions, where the reviewer runs the code and explores all possible code paths alongside the author&lt;/li&gt;
  &lt;li&gt;For any endurance race - finding someone or group of people doing the same race on a similar timeline and training together on at least a weekly cadence&lt;/li&gt;
  &lt;li&gt;For spiking on a new technology or language - meeting weekly or more to pair-program, review useful resources and discuss difficult concepts&lt;/li&gt;
  &lt;li&gt;For improving an athletic weakness - finding someone with the same weakness (ex. backstroke is weakest stroke for an individual medley swimmer, swimmer sees improving backstroke as easiest way to faster time) and crafting group training plan to make weakness strength&lt;/li&gt;
  &lt;li&gt;For changing a major issue in a codebase - devoting one or two days of the week to talk about the issue, brainstorm solutions and prototype solutions (through pair-programming) together. Importantly, the solution here would be group-created versus individual-created, which in my opinion is one reason resistance crops up for good ideas on introducing new patterns into a codebase and deprecating old patterns&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the very least, I think it’s worth exploring whether the group effect will work in domains that have traditionally been one-person activities.&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/09/15/the-group-effect.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/09/15/the-group-effect.html</guid>
      </item>
    
      <item>
        <title>React's act</title>
        <description>&lt;p&gt;One challenge of writing React tests is knowing where you are in the component lifecycle. Is the component mounted? In the middle of a state update? Has it received props? Answering these questions determines what the tester should assert. Through a colleague at work, I recently found React &lt;code class=&quot;highlighter-rouge&quot;&gt;16.9&lt;/code&gt; includes an &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; function that can take an &lt;code class=&quot;highlighter-rouge&quot;&gt;async&lt;/code&gt; callback.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; itself, however - which stands for the &lt;a href=&quot;http://wiki.c2.com/?ArrangeActAssert&quot;&gt;Arrange-Act-Assert testing pattern&lt;/a&gt; existed far before &lt;code class=&quot;highlighter-rouge&quot;&gt;16.9&lt;/code&gt;, and allows you to guarantee all state updates have completed before making assertions on a component. &lt;a href=&quot;https://reactjs.org/docs/testing-recipes.html&quot;&gt;From the docs&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When writing UI tests, tasks like rendering, user events, or data fetching can be considered as “units” of interaction with a user interface. React provides a helper called act() that makes sure all updates related to these “units” have been processed and applied to the DOM before you make any assertions…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I recommend the aforementioned &lt;a href=&quot;https://reactjs.org/docs/testing-recipes.html&quot;&gt;testing recipes&lt;/a&gt; page for a full walkthrough of &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; examples as well as &lt;a href=&quot;https://github.com/threepointone/react-act-examples/blob/master/sync.md&quot;&gt;Sunil Pai’s examples&lt;/a&gt; - Sunil originally authored the &lt;a href=&quot;https://github.com/facebook/react/pull/14853/files&quot;&gt;PR&lt;/a&gt; that acted &lt;code class=&quot;highlighter-rouge&quot;&gt;async&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt;. For this blog post, I want to walk through a simple example that will show the potential value of this testing utility.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note - all examples below can be found in this &lt;a href=&quot;https://codesandbox.io/s/1-react-act-use-4ke1t?fontsize=14&amp;amp;previewwindow=tests&quot;&gt;CodeSandbox&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with a sync &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt;. Consider a component with just a &lt;code class=&quot;highlighter-rouge&quot;&gt;useEffect&lt;/code&gt; call that sets a value on the component:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/34a93255596772fa1942757359ff371d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The difference between using &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; and not using &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; is the test waiting on the callback inside &lt;code class=&quot;highlighter-rouge&quot;&gt;useEffect&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/934bb121200a78227578967b1a85494d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;async&lt;/code&gt; act - again, newly released in React &lt;code class=&quot;highlighter-rouge&quot;&gt;16.9&lt;/code&gt; - allows you to wait on promises to resolve and state updates to complete.&lt;/p&gt;

&lt;p&gt;Here’s what an async component might look like…&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/c652ac47b58bc3402d80887459f9ba26.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;…and the differences between sync and async &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/1b13a6b1da5b3294864bd257f924480e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Importantly, &lt;code class=&quot;highlighter-rouge&quot;&gt;enzyme&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;react-testing-library&lt;/code&gt; are already pulling &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; into their API and have means of accessing it (or should have in the future), so if you’re using either library, the above boilerplate isn’t necessary. That said, I found it useful to go through this exercise to get a sense of how &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; works.&lt;/p&gt;

&lt;p&gt;I’m extremely excited React offers this utility - it’s extremely useful in fixing things like intermittently failing tests that rely on promises resolving in some fixed amount of time (which hopefully never existed in your test suite anyways). React testing forces you to understand the lifecycle of your components and how they handle state. &lt;code class=&quot;highlighter-rouge&quot;&gt;act&lt;/code&gt; is a great way to enhance that understanding.&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/08/11/react-act-as-valuable-testing-tool.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/08/11/react-act-as-valuable-testing-tool.html</guid>
      </item>
    
      <item>
        <title>ESG Investing Skepticism</title>
        <description>&lt;p&gt;&lt;em&gt;Note: I want to credit &lt;a href=&quot;https://www.aqr.com/Insights/Perspectives/Virtue-is-its-Own-Reward-Or-One-Mans-Ceiling-is-Another-Mans-Floor&quot;&gt;Cliff Asness&lt;/a&gt; and the folks over at &lt;a href=&quot;https://diligent-dollar.com/2018/07/18/why-do-sin-stocks-outperform/&quot;&gt;Diligent Dollar&lt;/a&gt; for many of the ideas in this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’ve been reading about Environmental, Social and Corporate Governance (ESG) investing recently and I think the topic has such widespread consensus support it’s worth taking a more skeptical approach. First - a quick disclaimer: I am still a supporter of ESG investing and as a rule of thumb will not invest in companies that I’m not morally aligned with. That said, I don’t think ESG investing is the most effective mechanism to promote companies with positive missions, and I think in some cases it can have the opposite effect of what investors intend.&lt;/p&gt;

&lt;p&gt;The reason for my ESG skepticism is that changing asset prices – what ESG does – is very different than changing company cash flows. The impact of changing asset prices based on criterion other than cash flow has unknowable effects. ESG allocates capital based on the murky notion of environmental or social responsibility, which is impossible to measure. For instance, is it responsible to sell carbon credits to polluting companies? Is it responsible to invest in nascent technologies that have unproven economics but could have some positive environmental impact? Measuring ESG goodness is hard.&lt;/p&gt;

&lt;p&gt;A quick overview of how ESG investing changes asset prices. Investors who support ESG-causes invest in ESG equities and therefore increase the price of these stocks. Conversely, they either don’t buy the stock of or even short anti-ESG equities, stagnating or driving down these equity prices. The same thing could also happen with bonds, warrants or other financial instruments companies use to raise capital.&lt;/p&gt;

&lt;p&gt;Importantly, changing these asset prices does impact companies’ ability to raise capital. Higher stock prices mean companies can raise more equity for the same amount of shares. Higher stock prices thus decrease a company’s cost of equity, which along with cost of debt is used to determine its cost of capital.&lt;/p&gt;

&lt;p&gt;If you believe corporate finance theory plays out in the real world, companies will only invest in projects that have a higher return than their cost of capital. Not doing this means they lose money on projects. Take a look at the little example below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/cash-flows-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.google.com/spreadsheets/d/12pxr7ZRNVKK2wZ04EEAfGFL90_EZho_nFXWdFFcsJSY/edit?usp=sharing&quot;&gt;Spreadsheet link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Assuming cash flow is re-invested each year, cost of funds going from 10% to 20% makes a 5 year project that returns 25% go to -25%.&lt;/p&gt;

&lt;p&gt;ESG investing lowers the cost of capital for ESG companies and increases it for anti-ESG ones. As a result, more projects now look attractive for “good” companies and less projects are worth investing in for “bad” companies.&lt;/p&gt;

&lt;p&gt;So what could happen as a result of the changed cost of capital across the board? ESG companies will take more risk since their hurdle rate is lower. In a best case scenario, the added risk plays out well and these companies see their returns exceed the low cost of capital. In the worst case scenario, these risk-on projects bear no fruit and companies waste the capital they raised. Investors lose trust and the ESG halo effect disappears. As an aside, depending on who you talk to this scenario has already played out - check out discussions about &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Green_Bubble&quot;&gt;The Green Bubble&lt;/a&gt; and implosions of scores of green-energy companies.&lt;/p&gt;

&lt;p&gt;On the other side, anti-ESG companies will invest in fewer projects given a higher cost of capital. Cash from earnings as a result will have fewer places to go. It’s even possible it will have no projects to go to, in which case management can keep it on balance sheet or return it to shareholders via dividends or buybacks. Again, depending on who you talk to, this has already happened with public tobacco companies. These companies generally have significantly-higher-than-average dividends and free cash flow yields. Several of them have even &lt;a href=&quot;https://diligent-dollar.com/2018/07/18/why-do-sin-stocks-outperform/&quot;&gt;significantly outperformed&lt;/a&gt; relative to market averages.&lt;/p&gt;

&lt;p&gt;One more note on anti-ESG companies - as long as there are investors who don’t mind investing in anti-ESG companies, depressing their stock prices always opens the door to such investors buying assets for pennies on the dollar and taking them private. These investors then can take the cash flow from the assets and use them for whatever causes they want. Said another way, artificially depressing anti-ESG asset prices opens the door for investors to come in and make outsized profits.&lt;/p&gt;

&lt;p&gt;So do I have a better proposal for promoting ESG causes than ESG investing? It’s easier said than done, but I think targeting cash flow directly is the best way to reward ESG companies and punish anti-ESG companies in the long run. At a consumer and investor level, this is as simple as deciding to not buy goods or services from anti-ESG and buy as much as possible from pro-ESG. If you’re a supplier, consultant or someone who sells to companies,  it’s discounting your ESG customers. These behaviors are guaranteed to impact the bottom lines of companies as opposed to changing their cost of capital. Better yet, they change the return rate on existing and new projects in a way not tied to the market (cost of equity and debt are both linked to market forces).&lt;/p&gt;

&lt;p&gt;Think of it this way - if a project returns -1% instead of 5%, no cost of funds will justify it. The way to get bad companies to face this -1% is to stop buying their products and make them face higher costs for their inputs. Conversely, if a project returns 10% instead of 5%, suddenly a cost of funds of up to 9% and change is acceptable. ESG companies now can access all types of new capital since their expected return from available projects is higher.&lt;/p&gt;

&lt;p&gt;In conclusion, I’m less supportive at this point of writing blank checks to ESG-companies than I am of buying products that were made in a socially and environmentally responsible manner. ESG investing is a very blunt instrument for changing how companies do business compared to the sharp instrument of changes in customer and supplier behavior.&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/04/20/esg-investing.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/04/20/esg-investing.html</guid>
      </item>
    
      <item>
        <title>AWS Explained to Shareholders From a Developer</title>
        <description>&lt;p&gt;There has been significant discussion recently around the following revelation in &lt;a href=&quot;https://www.sec.gov/Archives/edgar/data/1759509/000119312519059849/d633517ds1.htm&quot;&gt;Lyft’s S1&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In January 2019, we entered into an addendum to our commercial agreement with AWS, pursuant to which we committed to spend an aggregate of at least $300 million between January 2019 and December 2021 on AWS services. If we fail to meet the minimum purchase commitment during any year, we may be required to pay the difference, which could adversely affect our financial condition and results of operations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Many reactions I’ve seen to this post have expressed surprised at this dollar amount and suggested Lyft should build their own data center (&lt;a href=&quot;https://twitter.com/MohapatraHemant/status/1102401615263223809&quot;&gt;which was nicely rebutted here&lt;/a&gt;) or switch to another cloud provider.&lt;/p&gt;

&lt;p&gt;In addition, AWS CEO Andy Jassy &lt;a href=&quot;https://www.cnbc.com/2019/02/28/aws-ceo-andy-jassy-its-really-easy-to-lower-prices.html&quot;&gt;was recently on Mad Money&lt;/a&gt; and revealed AWS has a $30 billion run rate. The CNBC article on the appearance notes:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AWS has become a crucial part of Amazon’s overall business. In 2018, the unit generated $25.66 billion in revenue, or 11 percent of Amazon’s total sales, up from 10 percent of overall revenue in 2017. Growth at AWS accelerated to 47 percent last year from 43 percent in 2017.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most significantly, Jassy readily admitted to constant price decreases over the years in the various AWS product offerings:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“It’s actually really easy to lower prices,” Jassy told Jim Cramer on CNBC’s “Mad Money” on Thursday. “It’s much harder to be able to afford to lower prices.” In the past decade, AWS has cut prices 70 times, he said.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This appearance, combined with Lyft’s S1, have sparked various attempts to value AWS as a standalone business, determine the extent of AWS’ competition, size the total cloud market, etc. Jassy’s comments also have worried shareholders that AWS margins could materially decline.&lt;/p&gt;

&lt;p&gt;This post is an attempt to offer some thoughts on AWS spend from a developer perspective. Lost in much of the financial analysis of AWS is how developers use it on a day-to-day basis. This piece of information is important in doing financial analysis because it can offer insights into the costs of switching cloud providers and Amazon’s probability of retaining customers. I’m not suggesting this post could be used to quantify retention rates or market share, but that some of the information may be a good starting point in terms of how financial modelers think about AWS’ business.&lt;/p&gt;

&lt;p&gt;As background, I’m a developer who owns my own consulting company and have worked with a variety of clients that have unique AWS setups. Each client has unique tech needs based on their unique business models and growth stage. Each one picked AWS as their cloud provider.&lt;/p&gt;

&lt;p&gt;Notably, not a single one has ever seriously raised the issue of AWS pricing or wanting to switch to Google Cloud Platform (GCP), Microsoft Azure or another service. Again, this is solely my experience, but from a developer perspective, many solutions to problems discussed on blogs, at conferences or in person assume that AWS is the default cloud platform and speak about solutions with a specific set of AWS services in mind. This is the strength of the ecosystem at work.&lt;/p&gt;

&lt;p&gt;As an example, serverless architectures have recently become quite popular in the developer community. Serverless is a term that means from a cost perspective fees come in when useful “work” is occurring on a server rather than when time is elapsing. As it pertains to AWS, this is the difference between using an EC2 instance (a service that is priced per unit of time depending on the server specs) and &lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;AWS Lambda&lt;/a&gt; (a service that is priced closer to per unit of work (ex. per request)).&lt;/p&gt;

&lt;p&gt;Many developers when they talk about serverless architectures are usually talking about AWS Lambda. It’s very common to hear suggestions to the effect of “And you could write a Lambda task to do such-and-such and then…” AWS has made Lambda easy to pair with a number of its 100+ services, allowing developers to schedule Lambda tasks to occur immediately after file uploads to a specific S3 bucket (its storage service), entry of an item into SQS (its queuing service) or even specific patterns in Cloudwatch (its logging service). Serverless is as much a discussion about AWS as it is the broad idea of paying per unit of work and not per unit of time.&lt;/p&gt;

&lt;p&gt;Indeed, AWS is aware of this discussion and plays into the developer enthusiasm for serverless. Let’s look at an example directly from Amazon’s docs on serverless architectures:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/aws_infra_advertising.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To be clear here — to develop a simple weather app, AWS is suggesting pulling in four different AWS services — S3, API Gateway, Lambda and DynamoDB. Code will have to be written for each service and their interactions monitored. The costs will have to be tracked and made understandable for business stakeholders. This setup may even need to be rebuilt several times as the needs of the weather app become more clear.&lt;/p&gt;

&lt;p&gt;The paragraph above may seem crazy to you, but rest assured it is actually less crazy than real life. Only four AWS services seems pretty low in the context of apps I’ve worked on.&lt;/p&gt;

&lt;p&gt;I make this point to demonstrate that AWS is deeply ingrained in developer culture and becoming more deeply ingrained. It is a staple of how developers think about scaling operations and has entered developer lexicon for the foreseeable future.&lt;/p&gt;

&lt;p&gt;Changes in AWS change how developers think about the future and what’s possible. A release of a new AWS service or change to an existing service will trigger thousands of blog posts, training seminars and discussions. As a result, AWS exerts control over the future of software, and by extension its cost, performance and maintainability.&lt;/p&gt;

&lt;p&gt;Going back to the example of Lyft, I take issue with analysis that hones in on the $300 million AWS bill when the more important consequence is the complexity associated with this bill. Notice how Lyft never breaks down the spend itself. Is it serverless or server-based? Which AWS services make up the majority of the spend? How much time do Lyft developers currently spend writing code around AWS, and what would be the cost to switch platforms?&lt;/p&gt;

&lt;p&gt;As a result, I seriously question analysis that attempts to treat AWS spend as a per-transaction rake for Amazon. Credit card scanners are a commodity and not a core piece of company infrastructure heavily customized by full-time employees.&lt;/p&gt;

&lt;p&gt;While companies can and do model AWS costs with good accuracy, what I doubt is modeled nearly as well is the amount of time developers spend customizing and tuning AWS services. I’ve spent hundreds of hours in total reading AWS documentation, blog posts, source code from libraries that use AWS, talking to AWS sales reps and reading AWS message boards. It’s a key part of my job and I’m often hired with my knowledge of AWS specifically in mind.&lt;/p&gt;

&lt;p&gt;To underscore that AWS is not a commodity, I want to offer some more details on the costs in developer time of switching to another cloud provider.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The subject of security on AWS is enormous; there is a &lt;a href=&quot;https://d1.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf&quot;&gt;74 page whitepaper on it&lt;/a&gt;. There are consulting shops that specialize in this subject and charge absurd amounts of money to review your security stack. AWS’ security model requires knowledge of their &lt;a href=&quot;https://aws.amazon.com/iam/&quot;&gt;Identity and Access Management&lt;/a&gt; service and the security settings associated with each service (this could be security rules for EC2 or read/write permissions on S3). Rebuilding this security model in another platform would be non-trivial.&lt;/p&gt;

&lt;p&gt;Additionally, if your company has government clients, then storing their data in the cloud will require &lt;a href=&quot;https://aws.amazon.com/compliance/fedramp/&quot;&gt;FedRAMP&lt;/a&gt; certifications. The AWS docs note:&lt;/p&gt;

&lt;p&gt;Cloud service providers who want to offer their products and services to the US government must demonstrate FedRAMP compliance.&lt;/p&gt;

&lt;p&gt;Any company running their own data center would have to separately apply for these certifications, which can take in excess of a year to get. AWS is certified for most of its core offerings. Security within organizations, between organizations, between organizations and consumers and organizations and the government is handled in a unique way by AWS, further de-commoditizing it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think the average non-developer would be shocked if they searched a company codebase for references to AWS. I’ve yet to see a codebase that does not pull in a variety of AWS libraries and structures significant amounts of code around how these libraries work. Changing all this code would be non-trivial and would definitely require significant re-writes that would pull developers off feature work. This is not find-and-replace.&lt;/p&gt;

&lt;p&gt;There is even a robust ecosystem of companies that write code that target the various AWS services and make money off it. Heroku (a Y Combinator funded company that was purchased by Salesforce in 2010) runs many of its servers on AWS. Heroku is the middleman between thousands of companies and AWS. Many code registries tap into AWS to pull down third-party code. These same registries may also rely on Heroku or AWS middlemen. Even if a company switches its own cloud infrastructure to another provider, there is most likely still an AWS dependency in the code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Developer Workflow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Much of what a developer does day to day in my experience involves using various AWS services. Some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reviewing logs or performance metrics to determine the exact circumstances in place when a bug, outage or other event occurred. Developers can use this information to better determine the cause of the event&lt;/li&gt;
  &lt;li&gt;Monitoring, downloading or manipulating images, video, audio or other files internal users or customers uploaded into S3 (recall this is Amazon’s Simple Storage Service). S3 also can be used to host entire websites; in this case, the code for the website itself is hosted on S3&lt;/li&gt;
  &lt;li&gt;Creating, updating or deleting security rules to protect a company’s digital assets. One example of this is the inbound and outbound security rules on an EC2 instance (said another way, a server) that determine which IP or IP ranges the server can receive data from or send data to. Depending on their seniority or job responsibilities, developers may also be responsible for managing other developers’ permissions on various AWS services (the relevant AWS service here is the aforementioned IAM)&lt;/li&gt;
  &lt;li&gt;Configuring services that drive some critical part of a company’s product. AWS has services for machine learning, video games, the internet of things and even satellites. Each has hundreds if not thousands of dials you can turn for your specific use case
Jeff Bezos in a &lt;a href=&quot;https://www.youtube.com/watch?v=6nKfFHuouzA&quot;&gt;2008 presentation&lt;/a&gt; to a Y Combinator batch overviews a number of real world AWS use cases. This video is definitely worth a watch if you’re interested in how Bezos views AWS’ value add (notably, he speaks in depth about Blue Origin’s use of AWS)
All of these tasks to some degree require familiarity with the AWS UI and how to achieve certain tasks (this is mockingly known as “click-ops” in some circles). Devs would need to retrain themselves in all these tasks, some of which would no longer exist or exist in very different forms to accommodate a switch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, when the UI becomes cumbersome, developers will automate repetitive tasks through the AWS &lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;Command Line Interface&lt;/a&gt; (CLI), which also requires significant research and practice to get productive in. The cost of a switch from a developer workflow is to uproot how developers interact with the cloud from the browser and within code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hiring&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In my experiences as both an interviewer and interviewee, AWS has been a key part of hiring. I’m not judging whether this is right or wrong, but rather commenting that interviewers have asked me about it and interviewees have brought it up unprompted. There is an &lt;a href=&quot;https://aws.amazon.com/certification/&quot;&gt;AWS Certification program&lt;/a&gt; and I’ve been asked whether I’m certified and seen certs listed on resumes I’ve reviewed. Choosing GCP or Azure as your company’s cloud provider will likely increase the amount of time it takes for an average developer to come in and get comfortable with the company’s cloud setup.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In closing, I want to offer my thoughts on two questions that I think are extremely relevant for analyzing AWS spend and valuing AWS as a business.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Are cloud services a commodity?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Absolutely not. Here’s the definition of commodity from Merriam-Webster (note I’m using the third definition, which I think most applies to economics):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A good or service whose wide availability typically leads to smaller profit margins and diminishes the importance of factors (such as brand name) other than price&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I do not believe cloud services in very specific forms with very specific interactions — like the aforementioned weather app AWS services — are a commodity. That said, GCP has tried its best to map each AWS service one-to-one with its own.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/gcp_comp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not convinced at all that migrating is as simple as consulting this table and writing the migration to go from one service to the other.&lt;/p&gt;

&lt;p&gt;Building the aforementioned weather app in the way AWS recommends requires knowledge of four specific services. Yes, these services could be in part replicated with GCP or Azure, but it would take significant time and effort to rebuild it and there’s no guarantee the cost-savings would be significant. For commoditized goods, switching suppliers is trivial. From a code perspective, this is find-and-replace. Switching cloud services is anything but find-and-replace.&lt;/p&gt;

&lt;p&gt;Were this not true, I don’t think AWS’ competitors would author comprehensive guides on migrations. If you google “migrate aws to google cloud”, here are the first four results that come up:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/migration_search.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Google clearly has skin in the game and has spent time and money promoting the idea that switching is possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Is it worth switching from AWS to another cloud provider to save money or starting a company and using another cloud provider?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I did some research on migrating from AWS to GCP and what I found confirmed that this is a difficult undertaking. It’s notable here that complete migrations are not always the best choice for most companies — some companies are diversifying their cloud providers and picking the right tool for the right job. There is no doubt in my mind some Azure or GCP services make more sense than their AWS equivalents for specific companies in specific situations. That said, an 100% migration likely entails spending lots of times in areas where performance and cost improvement would be minimal. Some cloud services to be fair are commodities — how much of an improvement is your company going to get from moving from AWS Simple Storage Service to Google Cloud Storage?&lt;/p&gt;

&lt;p&gt;For new companies, I tend to think cost-cutting is less important than product-market fit. Discovering product-market fit is going to be easier with a cloud provider that has a rich ecosystem of libraries and documentation, as well as a large potential talent pool for working with those services.&lt;/p&gt;

&lt;p&gt;In conclusion, I think more discussions about margin compression and market share at AWS need to take into account what a critical role AWS plays in the day-to-day life of developers. Margin compression in part assumes that commoditization drives down prices — I’ve tried to rebut the idea here that AWS services are commodities. I’ve also tried to demonstrate that switching providers is non-trivial. When migrations do happen, they may be piecemeal and not complete infrastructure changes.&lt;/p&gt;

&lt;p&gt;Modeling cash flows requires making significant assumptions even for the most simple of businesses. AWS is as complicated a business as it is a product, and I would caution current and future shareholders against unfounded assumptions.&lt;/p&gt;

&lt;p&gt;Disclaimer: I am an AMZN, GOOGL and MSFT shareholder. I am an AWS, GCP and Azure customer.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/03/11/aws-for-shareholders.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/03/11/aws-for-shareholders.html</guid>
      </item>
    
      <item>
        <title>Exploring TypeScript Generics</title>
        <description>&lt;p&gt;I’ve been using TypeScript on several projects and wanted to jot down some quick thoughts on generics. To be clear, generics are not a new programming language construct and existed in languages liked C# and Java decades before TypeScript. That said, I find generics interesting in the context of TypeScript because I see and use them so often.&lt;/p&gt;

&lt;p&gt;First, what are generics? I think of generics as a way to represent types without explicitly defining a type. A generic type is generic in that it’s like a function parameter- able to represent anything. Also like a parameter in a function, its value can be passed as an argument (via &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;SomeType&amp;gt;&lt;/code&gt;) and referenced throughout the body of the function. This quality is one of many reasons generics are more powerful than simply using the &lt;code class=&quot;highlighter-rouge&quot;&gt;any&lt;/code&gt; type. &lt;a href=&quot;https://www.typescriptlang.org/docs/handbook/generics.html&quot;&gt;The official TypeScript docs on generics&lt;/a&gt; are an excellent resource on when to use generics, and I highly recommend them.&lt;/p&gt;

&lt;p&gt;Below are some use cases for generics I’ve found helpful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Promises&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In TypeScript, promises can be initialized such that they “lock” a generic into a type:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/promise.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Microsoft/TypeScript/blob/master/lib/lib.es2015.promise.d.ts#L33&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;Promise&lt;/code&gt; source&lt;/a&gt; makes the warning above possible. IDEs use the source to determine that the callback in the promise constructor must return something of type &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;PromiseLike&amp;lt;T&amp;gt;&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;undefined&lt;/code&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; in this case is number. Note that PromiseLike here is a &lt;a href=&quot;https://github.com/Microsoft/TypeScript/blob/master/lib/lib.es5.d.ts#L1376-L1384&quot;&gt;separate type&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the case of promises, I find generics useful because I can gain an understanding of what an async function resolves to without having to look at source code. Consider the following non-TypeScript example:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/fa6a4b370cfe1292d3e9db98b96860c6.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Now check out the TypeScript equivalent:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/a77c068e8e8d41191d434077b95cff8e.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Only in the second example can the developer know with some confidence that the promise resolves to a &lt;code class=&quot;highlighter-rouge&quot;&gt;UserProfile&lt;/code&gt; (well, or &lt;code class=&quot;highlighter-rouge&quot;&gt;undefined&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;PromiseLike&amp;lt;UserProfile&amp;gt;&lt;/code&gt;). Without TypeScript, it’s necessary to look at the function definition and the return value. Even then, the name &lt;code class=&quot;highlighter-rouge&quot;&gt;lookupProfile&lt;/code&gt; may be inaccurate depending on how it was implemented; TypeScript at least will fail to compile if the type returned is not a &lt;code class=&quot;highlighter-rouge&quot;&gt;UserProfile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Importantly, code changes in &lt;code class=&quot;highlighter-rouge&quot;&gt;lookupProfile&lt;/code&gt; are nicely handled by TypeScript generics. Let’s say the function can return an admin profile or a user profile - you can simply adjust the type to &lt;code class=&quot;highlighter-rouge&quot;&gt;Promise&amp;lt;UserProfile | AdminProfile&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;React Components&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;React.SFC&lt;/code&gt; - React’s stateless functional component - has a type definition like the below:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/dd9ae512d7d7eb26368e8f88a750ceda.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In the above, the type parameter is also passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;propTypes&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;defaultProps&lt;/code&gt; through &lt;code class=&quot;highlighter-rouge&quot;&gt;ValidationMap&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Partial&lt;/code&gt;, respectively, which also take generic arguments.&lt;/p&gt;

&lt;p&gt;Writing components without TypeScript might look like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/199b5d3097ef775243743a5392c51760.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;And now with TypeScript:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/9af73129bd46ac462f3f74987dbffaeb.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;While using &lt;code class=&quot;highlighter-rouge&quot;&gt;PropTypes&lt;/code&gt; is a perfectly valid option here and will offer some of the same benefits as TypeScript, you can still run a React application with components that are missing required props; it will just crash at runtime.&lt;/p&gt;

&lt;p&gt;With TypeScript, this error will happen at compile time, saving time and possibly a production bug that would never have been caught until users found it. Better yet, it offers a useful error message to developers who violate the component spec - let’s say in the case of a typo:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Type ‘{ message1: number; }’ is not assignable to type ‘IntrinsicAttributes &amp;amp; SomeProps &amp;amp; { children?: ReactNode; }’.
  Property ‘message1’ does not exist on type ‘IntrinsicAttributes &amp;amp; SomeProps &amp;amp; { children?: ReactNode; }’.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Again, &lt;em&gt;this error is not raised in the non-TypeScript case&lt;/em&gt;. The React component still renders and the div just has no text.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Apollo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Apollo client uses generics frequently throughout the result. I only highlight &lt;code class=&quot;highlighter-rouge&quot;&gt;ApolloQueryResult&lt;/code&gt; and the HOC &lt;code class=&quot;highlighter-rouge&quot;&gt;graphql&lt;/code&gt; in this discussion, but rest assured generics are heavily used in the Apollo codebase.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ApolloQueryResult&lt;/code&gt; (&lt;a href=&quot;https://github.com/apollographql/apollo-client/blob/master/packages/apollo-client/src/core/types.ts#L19-L25&quot;&gt;source here&lt;/a&gt;, returned from a query or mutation) takes a generic that describes data in a graphql response. The generic argument gets passed to the data property on the Apollo result. The advantages of this type are similar to the advantages of TypeScript promises (it actually acts as the &lt;a href=&quot;https://github.com/apollographql/apollo-client/blob/master/packages/apollo-client/src/ApolloClient.ts#L274&quot;&gt;generic argument&lt;/a&gt; a promise accepts).&lt;/p&gt;

&lt;p&gt;I’ve used Apollo without TypeScript and remember being frustrated with seeing different components extract different parts of &lt;code class=&quot;highlighter-rouge&quot;&gt;ApolloQueryResult&lt;/code&gt; - some components would utilize &lt;code class=&quot;highlighter-rouge&quot;&gt;loading&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;networkStatus&lt;/code&gt;, and / or &lt;code class=&quot;highlighter-rouge&quot;&gt;errors&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; especially was a difficult property to work with because knowing its shape - at least for me - generally required logging a response.&lt;/p&gt;

&lt;p&gt;TypeScript makes this logging unnecessary (well, less necessary - to be clear it’s impossible to know what the server will send back at compile time). Apollo provides a &lt;a href=&quot;https://codesandbox.io/s/github/apollographql/apollo-link-rest/tree/master/examples/typescript&quot;&gt;code sandbox&lt;/a&gt; here with a great example, part of which is copied in a gist below. Note that this example doesn’t actually use ApolloQueryResult and instead uses a similar prop from react-apollo that &lt;a href=&quot;https://github.com/apollographql/react-apollo/blob/apollo-client-2.0/src/types.ts#L66-L69&quot;&gt;gets added from &lt;code class=&quot;highlighter-rouge&quot;&gt;ChildProps&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/BenBrostoff/4e544db73630b6e488b58bca8272c930.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;graphql&lt;/code&gt; higher order component &lt;a href=&quot;https://github.com/apollographql/react-apollo/blob/master/src/graphql.tsx#L10-L31&quot;&gt;accepts two generic arguments&lt;/a&gt;, one of which describes the data from &lt;code class=&quot;highlighter-rouge&quot;&gt;ApolloQueryResult&lt;/code&gt;, and the other which describes the props passed to the component being wrapped by the higher order function.The graphql HOC takes the advantages described with &lt;code class=&quot;highlighter-rouge&quot;&gt;ApolloQueryResult&lt;/code&gt; and React component and combines them. It’s now obvious what the graphql server should be returning as well as what props the wrapped component expects.&lt;/p&gt;

&lt;p&gt;Side note - one interesting thing I found out about &lt;code class=&quot;highlighter-rouge&quot;&gt;ChildProps&lt;/code&gt; while researching this post is that it takes in two generics and from its constructor returns an &lt;a href=&quot;https://www.typescriptlang.org/docs/handbook/advanced-types.html&quot;&gt;intersection&lt;/a&gt; of types. This is a use case of generics that is extremely powerful; creating types that can intersect, unionize or do anything with multiple types to create new types.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;My brother and I have a multi-year debate going on about the value of code comments. I am a comment minimalist for many reasons, but the existence of type systems is the main point I fall back on. Compile time checks and good error messages in my opinion are a much better explanation of code than long comments.&lt;/p&gt;

&lt;p&gt;Generics offer the reusablity, type assertions and ease of understanding that comments cannot replicate. Most importantly, comments cannot prevent a class of bugs that could have been prevented at compile time.&lt;/p&gt;

&lt;p&gt;Generics make creation and reusability of types easy. The ability to create types that produce other types saves you a lot of typing - pardon the pun.&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2019/01/12/exploring-typescript-generics.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2019/01/12/exploring-typescript-generics.html</guid>
      </item>
    
      <item>
        <title>Stop Resulting</title>
        <description>&lt;p&gt;In 2018, the best lesson I learned was to stop creating narratives based on results.&lt;/p&gt;

&lt;p&gt;Creating a narrative based on results is what former professional poker player and World Series of Poker Champion Annie Duke calls “resulting” in her book &lt;a href=&quot;https://www.amazon.com/Thinking-Bets-Making-Smarter-Decisions-ebook/dp/B074DG9LQF&quot;&gt;Thinking In Bets&lt;/a&gt;. One example Duke returns to throughout the book is some poker players’ tendency to use the results of a hand to evaluate the decisions that took place during it.&lt;/p&gt;

&lt;p&gt;Players that fall into the trap of resulting often want to preserve a narrative that emphasizes their own (assumed) above-average skill. These players believe decisions made if they won a big hand were skill and not luck-based. Conversely, to preserve this narrative, these same players assume their losses were luck and not skill-based.&lt;/p&gt;

&lt;p&gt;In reality, luck plays a role in the outcome of any individual poker hand. As Duke notes in the book, if a player is 80% favored to win a hand before more cards are placed on the table, the same player losing the hand once all cards are out is not wrong, but part of the 20% chance another player wins.&lt;/p&gt;

&lt;p&gt;The outcome of a hand like that isn’t part of a narrative, but part of a statistical distribution. The player who lost the hand needs to place their decision to play before cards came out in the context of the 80%, and the one who won should recognize they only had a 20% chance of victory in the pre-flop situation.&lt;/p&gt;

&lt;p&gt;Resulters ignore or manipulate this context in order to draw a lesson from an outcome that fits a narrative. Resulting is clearly detrimental to learning because it fails to consider significant portions of the why behind decisions — views of uncertainty at the time the decision was made, and whether those views were accurate.&lt;/p&gt;

&lt;p&gt;My opinion — based on my own life — is that most junior and mid-level jobs prepare people to be resulters. The jobs I had early out of college involved implementing well defined specs in code (ex. hide the button when the user isn’t authorized) or making template-based Excel models (ex. reduce the cash flow 20% in the stress test). Quality of work was easy to measure — the code or the model did what was expected.&lt;/p&gt;

&lt;p&gt;The outcomes in the businesses I worked at were also easy to measure (whether I was looking at the right outcomes is for another blog post). Looking at user traffic or deals won or capital raised at what valuation was the scoreboard to me. So, my logical conclusion most times until this year was:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If I execute well, my company will have a higher probability of success&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, this may come off as controversial, but for the majority of jobs at the majority of companies, I believe this statement to be wrong. Most people at companies execute ideas created by others and execute them fairly well. Based on the ever-improving quality of tools in finance and web development — the two industries I’m most familiar with — good execution is becoming easier and easier.&lt;/p&gt;

&lt;p&gt;When was the last time you saw a start-up that did not have a flashy looking product or an impressive demo (signs of great execution)? Yet the statistics are clear — most start-ups fail. Clearly, execution isn’t the whole story, or success would be more common. It may not even be positively correlated with success, as evidenced by start-ups where unpolished prototypes representing a pivot saved the day, supplanting polished existing products.&lt;/p&gt;

&lt;p&gt;I started my own software consulting business this year and gained an appreciation for the impact of confusing process and outcomes. My first introduction to this idea was getting clients. Working through a combination of LinkedIn, AngelList, HN’s Who’s Hiring, networking events and friends and family, I found some introductory calls would go swimmingly and others would be a complete failure.&lt;/p&gt;

&lt;p&gt;My actions before the calls were the same — same research process, same introduction, mostly same pitch (with some tweaks over time as I learned more).&lt;/p&gt;

&lt;p&gt;I initially equated the bad outcomes to lack of skill and the good ones to skill, but over time it became clear that companies have varying degrees of acceptance to contracting. Some companies I encountered only had developers that were contractors; others only had full-time developers. There was no easy way to gain this information without doing the call in the first place.&lt;/p&gt;

&lt;p&gt;I now accept based on the data I’ve gathered that the percentage of companies willing to hire contractors for the services I provide is probably below 25%. This is not to say I haven’t tanked calls and performed well on others; it’s just that I hear the “we generally don’t hire contractors” line or some variation of it 75% of the time. Some of these companies can still be convinced, but the odds of success are far lower than a company with previous positive experiences with contractors.&lt;/p&gt;

&lt;p&gt;Another example — early in my search for clients, I found a majority of companies asking for help with devops, specifically Kubernetes. I had to pass on these opportunities as my Kubernetes experience is minimal. I became concerned that the only consulting opportunities might involve Kubernetes, despite the fact that the sample size at that point was probably five 30 minute discussions.&lt;/p&gt;

&lt;p&gt;I can say now with a full year of calls under my belt that help with Kubernetes or devops tasks represents a small proportion of the consulting asks I get. The spike in asks at the beginning was the result of a small sample size. Deriving information from this sample was ill-advised and a classic example of &lt;a href=&quot;https://medium.com/@bmb21/stop-tape-watching-f188df01b9b7&quot;&gt;watching the tape&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rejections by companies or opportunities I decline because I can’t help I used to see as a personal failure, and sometimes still do. But I think this is an insidious form of resulting that will hurt my business long-term, just as assuming knowledge of one JavaScript framework will be worth just as much today as it is in three years. Deep diving into whatever technology is popular or continually using the same tools can both be harmful when driven by knee-jerk interpretation of results. Execution without constant evaluation of what is being executed is, as discussed in Naval Ravikant’s recent podcast with Kapil Gupta, &lt;a href=&quot;https://startupboy.com/2018/12/25/the-truth-about-hard-work/&quot;&gt;hard work for hard work’s sake&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Changes in priorities and plans are evidence that leaders of a business are evaluating the probabilities of success for different strategies, be it correctly or incorrectly. Importantly, this view is different from what I used to believe. It used to hugely frustrate me when I worked on some complicated interface for months only for it be thrown away. I would attribute the code churn to bad communication between sales and engineering — an execution failure. Perhaps this explanation is accurate some of the time. Plans also change because new information forces strategy changes.&lt;/p&gt;

&lt;p&gt;Resulters blame execution and bad decisions for why certain strategies don’t pan out. They fail to recognize that their evaluation is based on outcomes. The execution was good if the sales person made the sale; the decision to play the hand was bad if the player lost the hand.&lt;/p&gt;

&lt;p&gt;Figuring out what needs to be done I now believe is a process that requires viewing process and outcomes separately. The risk of tying good and bad outcomes to skill through a narrative is rejecting useful strategies and embracing harmful ones. In 2019, I want to eliminate this risk by separating process and outcomes.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2018/12/27/biggest-lesson-2018.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2018/12/27/biggest-lesson-2018.html</guid>
      </item>
    
      <item>
        <title>EV is Everything</title>
        <description>&lt;p&gt;Expected value (EV) is the best mental model I know of for decision making. In this post, I want to explain how EV has begun to guide much of my decision making about the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is EV?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, let’s back up and give a broad overview of what expected value is and why it’s important. EV &lt;a href=&quot;https://www.google.com/search?q=expected+value&quot;&gt;per Google&lt;/a&gt; is “a predicted value of a variable, calculated as the sum of all possible values each multiplied by the probability of its occurrence.” I interpret this as a wordy way of saying EV is the chance something will happen multiplied by the payoff if it does happen.&lt;/p&gt;

&lt;p&gt;A coin flip is the easiest example to start with here, so if I offer you $10 if a coin turns heads, the EV is $5 (50% * $10, the outcome where heads comes up, plus 50% * 0, the outcome where tails comes up).&lt;/p&gt;

&lt;p&gt;This coin flip is what I’ve seen many people refer to as +EV, meaning the expected value is positive. This game worst case leaves you with the world unchanged half the time, and best case puts $10 in your pocket half the time. You should play this game forever because the average return is $5.&lt;/p&gt;

&lt;p&gt;However, let’s assume it costs $6 to play the game. In this case, the game is -EV; in the win case, EV is $2 in the winning case (50% * (10–6)) and -$3 in the losing case (50% * -6), so total EV is 2–3, or -$1. You should never play this game because the average return is -$1.&lt;/p&gt;

&lt;p&gt;In this simple coin flip example, notice the lever we changed to change the EV: what it costs to play the game. Paying anything more than $5 makes the above a -EV game. The other lever we have as the game makers is changing the odds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The same EV does not mean the same game odds&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can get to the same $5 and -$1 EV above using an unfair coin and different payoffs. If the coin has a 90% chance of turning up heads, a $5 EV is betting on heads with a ~$5.56 payoff ($5.56 * 0.9 === $5). The -$1 EV is charging $5 to play the game with the same payout (0.9 * (5.56–5) + -5 * 0.1 ~== -1). While the EV of both games is the same as in the first example, having a 50% chance of winning $10 is very different from a 90% chance of winning $5.55. Additionally, paying $5 to play the game is different from paying $6 — with higher dollar amounts, a 20% increase in the cost to play will price many people out.&lt;/p&gt;

&lt;p&gt;I think this idea becomes even more clear for games with longer shot odds. I was at a casino for Thanksgiving and had to wait an hour for the next available poker table, so I played roulette to pass the time. I was surprised to see the number of players simply placing bet after bet on a single number. Roulette offers a 36:1 payout on a single number hitting when the actual probability is 38:1. This is a negative EV game as a result. Let’s assume you have $190 to invest in 38 roulette turns at $5 each. If you’re comfortable with the 36% chance of losing your investment completely and 37% chance of losing 5% of your investment, there are some huge payoffs at low percentages.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/p_v_r_extreme.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Note that you can see the spreadsheet I used to make all graphs in this post &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1pcN1fYHKB-woSJFYbOqKhfuzvBEqNw4R0TGY_zAg3ng/edit?usp=sharing&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Moreover, you also have the opportunity to walk away any time you hit the number and reduce that $190 expenditure. To put this in perspective, if your number hits 2 times in a row ((1/38)², or ~0.07% chance), you’ve only put $15 at risk and won $335 for a 22.3x return. You should still never play roulette; it’s a negative EV game (-$10 in the example above).&lt;/p&gt;

&lt;p&gt;Let’s contrast this with a more even-odds approach to roulette. The EV is still the same (-$10), but the return profile of investing 5 bucks each on 38 turns looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/brostoff-blog/p_v_r.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I did not see anyone playing this particular roulette strategy, presumably because no one wants to play 38 turns to win or lose up anywhere between 0 and $100 more than ⅔ of the time. It’s more attractive for most players to have a low probability of a huge payout than a near 50% probability of a small payout. I actually asked my brother about this idea and he had a telling quote:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;My expectation is that I’ll lose some set amount of money I already decided on or win a lot of money at a casino. Winning a little bit of money isn’t why people go.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;But real life is more complicated&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As with pretty much every probability example that gets presented to people, the easy response about why it’s non-applicable to real life is that real life is complicated. Nassim Nicholas Taleb, Jonathan Bales and others have written about two types of EV, so I’ll overview both. I’m going to call these hard EV and soft EV:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Hard EV&lt;/em&gt; — EV you can confidently calculate that depends on some physics laws that are random (gravity and other forces move a roulette ball to its position — as a side note, Claude Shannon attempted to calculate the position of a roulette ball and bet based on the expected position by using some radar-gun like devices hidden beneath his clothes at casinos — this is detailed in the great book &lt;a href=&quot;https://www.amazon.com/Fortunes-Formula-Scientific-Betting-Casinos-ebook/dp/B000SBTWNC/ref=sr_1_1?ie=UTF8&amp;amp;qid=1543171941&amp;amp;sr=8-1&amp;amp;keywords=fortune%27s+formula&quot;&gt;Fortune’s Formula&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Soft EV&lt;/em&gt; — EV based on human events. Sports books, insurance companies, banks and others that price odds impacted by humans back into EVs by seeing what people bet when they offer opening odds on different events&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I believe the principles of dealing with hard EV are applicable to soft EV, as long as we admit that our attempts to price soft EV accurately are going to fail. When those attempts do fail, we can use the information gained to price EV more accurately next time. &lt;a href=&quot;https://medium.com/@bmb21/the-value-of-wrong-and-right-guesses-998a62bfc142&quot;&gt;My recent post on Mastermind&lt;/a&gt; explores this idea. Moreover, failed attempts to price EV can lead us to pursue +EV generating projects, which is at the heart of the next idea I want to discuss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fight for every last scrap of EV&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Anyone who has played casino games has definitely done the thing where you go “If only that thing didn’t exist, this would be a great game.” This thought is really just expressing that casinos always do at least one thing in games they offer that makes the game -EV. This is the green 0 and 00 in roulette, the rules around when the dealer hits in blackjack, the programming logic in when slot machines pay out, etc. Casinos are acutely aware of this fact and will actually offer scraps of EV to high rollers or improved odds when they first open.&lt;/p&gt;

&lt;p&gt;So, if a small change in odds or payoffs make hard EV games worth playing, I think the same is no less true for soft EV. As someone who has been around start-ups the majority of their career, I have come to believe this is the only thing start-ups do; continually search for +EV activities.&lt;/p&gt;

&lt;p&gt;Let’s consider a simple start-up that sells widgets to OEMs that make some part of manufacturing more efficient (maybe it’s an IoT device with machine-learning capabilities or something). The company may estimate there is a 60% chance some big tech company comes in and manufactures the devices for 30% cheaper. They then can start to price how much they should raise in capital to get their devices to be 30% cheaper.&lt;/p&gt;

&lt;p&gt;To continue this example, the company could also decide to try to move the goalposts on the 60% chance. They could partner with a big tech company and build in contract language that prevents them from manufacturing competing devices; they could create their own operating system or hardware to make the device harder to replicate; they could decide to migrate out of the widget business because raising capital is too expensive relative to their EV guesstimate.&lt;/p&gt;

&lt;p&gt;Just like small changes in casino odds make the games worth playing, start-ups making small changes completely change what is +EV and what is -EV. With soft EV, human actions can wildly change odds and payoffs, so in my mental model I equate new projects and strategies at start-ups to casinos changing the odds and payoffs. From a career perspective, how well start-ups manage EV is how I evaluate whether I want to stay or go; if the odds are continually moving toward a place where the game is worth playing, stay; if not, go.&lt;/p&gt;

&lt;p&gt;But then how do you evaluate how people manage EV?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drop your state-of-the-world evaluations for first principles, especially for long-shots&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There’s a story Nassim Nicholas Taleb tells often about how he was at a meeting where traders at his firm said what they thought would happen in the market. Taleb expresses his opinion, and one of his colleagues ask why his portfolio isn’t positioned that way. Taleb responds that what he thinks is irrelevant; all that matters is what the market is mispricing.&lt;/p&gt;

&lt;p&gt;I think a reasonable approach to evaluating soft EV — and how people / companies manage it — is to 1) forget what you think you know about the opportunity associated with EV and 2) try to price it from a first principles perspective. (1) improves the odds you stop working off a false collection of EVs. (2) forces creation of a new collection of EVs.&lt;/p&gt;

&lt;p&gt;Doing both 1) and 2) may lead to surprising conclusions, which leads me to my next point; unpopular stuff is most likely to be mispriced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When in doubt, prefer low information EV to high information EV&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I remember in grade school social studies learning about the concept of the catch-up effect. The catch up effect says that developing countries will grow faster than developed countries because there are easier gains to be made for the developing countries. A developing country for instance may realize a huge benefit from an infrastructure project that connects two cities; a developed country will have less of these opportunities because that same project was already completed. That infrastructure project may be a percentage point or more of GDP growth for the developing country. The developed country has no comparable way of boosting its GDP, so the developing country is likely to catch up in standard of living, median income, etc. over time.&lt;/p&gt;

&lt;p&gt;I think there is a real catch-up effect that exists for “cold” versus “hot” industries. Cold industries are what I consider to be businesses that are not getting attention from the media and very few people are playing in. You are unlikely to find lots of white papers or blogs on it online. In order to find information, you are probably going to have to read forums or maybe even (gasp!) pick up the phone and call someone.&lt;/p&gt;

&lt;p&gt;Hot industries are going to be reported on by most tech blogs, have a lot of capital coming in and have highly trained players that are pricing EV like their career depends on it. There are probably a few dozen best-selling books that talk about how the future is going to radically change because of Industry X.&lt;/p&gt;

&lt;p&gt;Cold industries are low information industries precisely because they’re “cold” — not popular. Hot industries are saturated with market entrants and information because they’re hot. Low information means more mispricings, because information is the only way we can calculate EV. Without information, we don’t know payoffs or probabilities.&lt;/p&gt;

&lt;p&gt;When viewed this way, I think the catch-up effect of cold industries relative to hot industries is real. I believe it’s a strong argument for playing in cold industries. When I’ve argued this point, I’m often told that I’m a contrarian. I actually think that’s an unfair characterization, because contrarian implies that you just like choosing the opposite way the crowd is going. Instead, I think letting EV serve as a mental model for decision-making leads to contrarian viewpoints. Having contrarian viewpoints is a goal worth pursuing because the future can and will have moments where low-information, cold industries become high(er) information, hot industries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EV is everything because the future is uncertain&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Being told you’re a contrarian is a good sign you’re pursuing a EV+ opportunity. We already know the world does not price EV perfectly (that casino was super-crowded on Thanksgiving no less) and has a built-in bias for what’s hot at the moment.&lt;/p&gt;

&lt;p&gt;EV+ opportunities are easier to come by in low information industries because there are fewer tools to know payoffs or odds. Said another way, it’s easier to scrap for EV because there are more scraps. When the future turns in a “crazy” way (or just a way the crowd didn’t expect), cold industries are more likely to rise and hot industries to fall because the crowd predicted something that did not happen. The contrarians are then better positioned for the future; the contrarians thus become what’s hot; and the cycle continues.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tim.blog/2018/09/25/howard-marks/&quot;&gt;Investor Howard Marks said in a recent podcast&lt;/a&gt; with Tim Ferriss that knowing where we are in the investment cycle is among the keys to calling the market correctly. Just like the investment cycle (boom and bust, high and low leverage), the EV cycle (hot and cold industries, high and low information) is part of the world we live in.&lt;/p&gt;

&lt;p&gt;I’m not certain what the future holds, but I know an EV-based view of it will lead to different conclusions than alternatives.&lt;/p&gt;
</description>
        <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2018/11/25/ev-is-everything.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2018/11/25/ev-is-everything.html</guid>
      </item>
    
      <item>
        <title>Guesses and Information</title>
        <description>&lt;p&gt;Learning from your wrong guesses is fundamentally different than learning from mistakes. In this post, I’ll explore why playing the board game Mastermind has given me a strong appreciation for the power of wrong guessing.&lt;/p&gt;

&lt;p&gt;First, I want to clearly differentiate between mistakes and guessing wrong. Mistakes can be avoided by logical thinking. For example, it’s a mistake to play casino games because they have negative expected value. You will lose money if you play on a long enough time horizon.&lt;/p&gt;

&lt;p&gt;Guessing about the state of the world isn’t a mistake; in fact, it’s something I believe we should do as often as possible. Guessing is necessary because it is often the only device we can use to gain information.&lt;/p&gt;

&lt;p&gt;This idea presented itself while playing Mastermind with one of my buddies this week. For those unfamiliar with the game, two players face off where one creates an ordered sequence of four pegs (which can be six different colors), and the other player guesses to find the combo. The guesser on each guess is given the number of exact peg guesses correct (right color, right place) and number of color guesses correct (right color, wrong place). In this way, the guesser gains information on every guess. As an aside, Donald Knuth developed an algorithm to determine the pattern in &lt;a href=&quot;http://www.cs.uni.edu/~wallingf/teaching/cs3530/resources/knuth-mastermind.pdf&quot;&gt;at most five guesses&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you think Mastermind is easy for the guesser, you’re not wrong. Each turn gets you closer to the four peg combo, and not solving the game in the eight allotted guesses (depending on what version you’re playing) most likely means you failed to use available information to your advantage.&lt;/p&gt;

&lt;p&gt;That said, it’s possible and likely to regress on the number of correct scores from turn to turn. Consider this scenario:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/redux-series/guess-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the first turn, the guesser is elated when they are given feedback of three exactly correct guesses. But which ones? It’s statistically unlikely on the second turn to &lt;em&gt;not&lt;/em&gt; regress. You have to guess about the state of the world because it’s impossible to know which three were correct.&lt;/p&gt;

&lt;p&gt;Here’s a good second guess:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/redux-series/guess-2a.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The guesser scores a “worse” score here, with only two right-color, right-position and one right-color, wrong-position, but has extracted valuable information:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Red is clearly a wrong color (or else they would get four scoring pegs)&lt;/li&gt;
  &lt;li&gt;One of the blue pegs is in the wrong position (or else they would have equaled the previous score)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From here, aiming to get the color scheme correct (by continually swapping out the red peg for other colors until the scoring changes) and getting the position correct (by moving the three blue pegs to different positions until three are correct again) are the clear strategies. Guessing wrong has delivered this information.&lt;/p&gt;

&lt;p&gt;This idea to me is the central lesson of Mastermind that extends to so many other areas of life: When we are most wrong, we have extracted more useful information than when we are close to right. The reason I think this is the case is that being close to right does not necessarily deliver any information on &lt;em&gt;why&lt;/em&gt; we are close to right.&lt;/p&gt;

&lt;p&gt;Flat-out wrongness at least shows you nothing is working; rightness forces you to guess on the factors that led to success. In this way, being right may not deliver useful information.&lt;/p&gt;

&lt;p&gt;Human psychology unfortunately is not conducive to embracing this lesson. Being right makes me feel smart and wrong makes me feel stupid. Hence, guessing right is dangerous because we may extract information that had nothing to do with being right. Guessing wrong makes it easy to abandon a venture because we just assume we have no potential to succeed due to some genetic or personal failure. In reality, there is no skill to guessing, so we should de-personalize successes and failures where luck plays a key role.&lt;/p&gt;

&lt;p&gt;The lesson here I think is to keep guessing after initial success to reject or confirm the &lt;em&gt;why&lt;/em&gt; of the right guess. Similarly, with wrong guesses, instead of writing ourselves off, our energy should be focused on extracting as much information as possible from the experience.&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2018/11/17/guesses-and-information.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2018/11/17/guesses-and-information.html</guid>
      </item>
    
      <item>
        <title>Planning Time for Internal Users</title>
        <description>&lt;p&gt;The data exposed in a customer-facing UI is not the data internal users care about.&lt;/p&gt;

&lt;p&gt;Yet, this data forms the basis for the abstractions developers create. Easy-to-use APIs and GUIs generally expose the data the customer can access. Exposing data not exposed to the customer — think user behavior, hyper-granular statistics captured by IoT equipment, growth rates over specific time periods — is not easy.&lt;/p&gt;

&lt;p&gt;The APIs, GUIs and abstractions dedicated to customer-facing data are rarely usable for internal data. This is a problem because internal users often have greater and more immediate data needs than customers. These users deserve as much attention as customers.&lt;/p&gt;

&lt;p&gt;Why? Internal data should be easy to access and analyze because it contains time-sensitive information about a company’s customers and its products that hold the key to growth. If internal users at a company can easily access data over any time period, they can determine customer behavior in response to changes in the product. Some real-world examples of data internal to a company revealing information essential to company strategy:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Buzzfeed shows &lt;a href=&quot;http://www.niemanlab.org/2017/09/buzzfeeds-strategy-for-getting-content-to-do-well-on-all-platforms-adaptation-and-a-lot-of-ab-testing/&quot;&gt;different titles and thumbnail art to different users&lt;/a&gt; for the same story, tracking the traffic from the different versions to optimize what it publishes&lt;/li&gt;
  &lt;li&gt;Intuit runs &lt;a href=&quot;https://www.fastcompany.com/3020699/why-intuit-founder-scott-cook-wants-you-to-stop-listening-to-your-boss&quot;&gt;real-world experiments within its products&lt;/a&gt; and pursues or abandons ideas based on the results&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/&quot;&gt;Amazon actively maps the relationship between site speed and sales&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Eric Ries refers to this concept as completing a feedback loop in the shortest possible amount of time and argues it is the key to how quickly a start-up can grow in &lt;a href=&quot;https://www.amazon.com/dp/B004J4XGN6/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1&quot;&gt;The Lean Startup&lt;/a&gt;. Investment in internal data is an investment in the customer.&lt;/p&gt;

&lt;p&gt;The internal data needs of a company change not on a Scrum schedule, but on a minute-to-minute or even second-to-second schedule. This is especially true in industries like fintech (think flash crash) and automotive (think actual crash), where being able to capture what happened in the space of a few milliseconds can save a company.&lt;/p&gt;

&lt;p&gt;Ability to analyze your data will determine the fate of your start-up. This is the number one reason to invest time in making data easier to share and analyze.&lt;/p&gt;

&lt;p&gt;How much time is enough time for teams to invest? 15 minutes is a start that could lead to perpetual gains. If you’re a Scrum Master, organize a short meeting with the developers who interact with your databases and business analysts who generate the most inbound. You can pre-wire this meeting by asking the business analysts to generate a list of their biggest pain points. You can ask the developers what they could do in one hour or less to start addressing these pain points. Something as simple as a cron job that e-mails internal users key data or a CSV that can feed pivot tables is a useful outcome from this meeting.&lt;/p&gt;

&lt;p&gt;If this meeting proves valuable, consider making it recurring and using it as a basis for small to medium-sized tasks that can be pulled in each sprint. When I’ve done similar exercises at previous companies, the first data-access related meeting has led to a host of issues being identified and even resolved in one day. In one instance, a co-worker who needed access to test data (only used internally) asked if there was any way he could periodically get data for one specific query. My company at the time had a GraphQL API with graphiql exposed in the test environment; I was able to give my co-worker graphiql access, and he was writing queries just with the GraphQL docs as a guide in 5 minutes.&lt;/p&gt;

&lt;p&gt;How much time will good data tooling save? Possibly years. Building internal data tools is no different than building features in that it is an investment in customer success. Buzzfeed, Intuit and Amazon in the examples above all used internal data to drive customer acquisition and retention. Additionally, strong data tooling impacts employee retention as business analysts expect data to be easy to access and analyze, and developers expect that their schedule to be free of fire-drills.&lt;/p&gt;

&lt;p&gt;Investments in improving data tooling pay long-term dividends. Finding time for them this sprint — and not three months into the product backlog — matters as much as building the product.&lt;/p&gt;

</description>
        <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
        <link>http://0.0.0.0:4000/2018/08/28/internal-users.html</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2018/08/28/internal-users.html</guid>
      </item>
    
  </channel>
</rss>